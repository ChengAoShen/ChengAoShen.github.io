<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Qingsong Wen on ChengAo Shen</title>
    <link>https://chengaoshen.com/authors/qingsong-wen/</link>
    <description>Recent content in Qingsong Wen on ChengAo Shen</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sun, 12 Oct 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://chengaoshen.com/authors/qingsong-wen/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>SVTime: Small Time Series Forecasting Models Informed by &#34;Physics&#34; of Large Vision Model Forecasters</title>
      <link>https://chengaoshen.com/en/publications/svtime/</link>
      <pubDate>Sun, 12 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/publications/svtime/</guid>
      <description>&lt;p&gt;Abstract: Time series AI is crucial for analyzing dynamic web content, driving a surge of pre-trained large models known for their strong knowledge encoding and transfer capabilities across diverse tasks. However, given their energy-intensive training, inference, and hardware demands, using large models as a one-fits-all solution raises serious concerns about carbon footprint and sustainability. For a specific task, a compact yet specialized, high-performing model may be more practical and affordable, especially for resource-constrained users such as small businesses. This motivates the question: Can we build cost-effective lightweight models with large-model-like performance on core tasks such as forecasting? This paper addresses this question by introducing SVTime, a novel Small model inspired by large Vision model (LVM) forecasters for long-term Time series forecasting (LTSF). Recently, LVMs have been shown as powerful tools for LTSF. We identify a set of key inductive biases of LVM forecasters &amp;ndash; analogous to the &amp;ldquo;physics&amp;rdquo; governing their behaviors in LTSF &amp;ndash; and design small models that encode these biases through meticulously crafted linear layers and constraint functions. Across 21 baselines spanning lightweight, complex, and pre-trained large models on 8 benchmark datasets, SVTime outperforms state-of-the-art (SOTA) lightweight models and rivals large models with 10^3 fewer parameters than LVMs, while enabling efficient training and inference in low-resource settings.&lt;/p&gt;</description>
    </item>
    <item>
      <title>From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?</title>
      <link>https://chengaoshen.com/en/publications/av4ts/</link>
      <pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/publications/av4ts/</guid>
      <description>&lt;p&gt;Abstract: Transformer-based models have gained increasing attention in time series research, driving interest in Large Language Models (LLMs) and foundation models for time series analysis. As the field moves toward multi-modality, Large Vision Models (LVMs) are emerging as a promising direction. In the past, the effectiveness of Transformer and LLMs in time series has been debated. When it comes to LVMs, a similar question arises: are LVMs truely useful for time series analysis? To address it, we design and conduct the first principled study involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across both high-level (classification) and low-level (forecasting) tasks, with extensive ablation analysis. Our findings indicate LVMs are indeed useful for time series classification but face challenges in forecasting. Although effective, the contemporary best LVM forecasters are limited to specific types of LVMs and imaging methods, exhibit a bias toward forecasting periods, and have limited ability to utilize long look-back windows. We hope our findings could serve as a cornerstone for future research on LVM- and multimodal-based solutions to different time series tasks.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
