<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ChengAo Shen</title>
    <link>https://chengaoshen.com/</link>
      <atom:link href="https://chengaoshen.com/index.xml" rel="self" type="application/rss+xml" />
    <description>ChengAo Shen</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 31 Dec 2099 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://chengaoshen.com/media/icon_hu13846342205060721621.png</url>
      <title>ChengAo Shen</title>
      <link>https://chengaoshen.com/</link>
    </image>
    
    <item>
      <title>⚽️Introduction to Prompt Engineering</title>
      <link>https://chengaoshen.com/blogs/introduction-to-prompt-engineering/</link>
      <pubDate>Mon, 15 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/blogs/introduction-to-prompt-engineering/</guid>
      <description>&lt;h2 id=&#34;basic-knowledge&#34;&gt;Basic Knowledge&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Prompt engineering&lt;/strong&gt; is a relatively new discipline for developing and optimizing prompts to efficiently use large lange models (LLMs) for a wide variety of applications and research topics. Researchers use prompt engineering to improve the safety and the capacity of LLMs on a wide range of common and complex tasks such as question answering and arithmetic reasoning. Developers use prompt engineering to design robust and effective prompting techniques that interface with LLMs and other tools.&lt;/p&gt;
&lt;p&gt;When designing and testing prompts, we typically interact with the LLMs via an API. Some paprameters will influence the performance of the model, below are some common settings that will come across when using LLMs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Temperature - This parameter controls the degree of randomness in the model&amp;rsquo;s output. A higher temperature will result in more random outputs, while a lower temperature will result in more repetitive outputs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Top P - This parameter controls the sampling strategy used by the model when generating multiple possible outputs. A higher value of Top P will result in more diverse outputs, while a lower value will result in more repetitive outputs.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;The general recommendation is to alter temperature or Top P but not both.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Max Length - This parameter controls the number of tokens the model generates in its output. Using a typical max length will help to prevent long or irrelevant responses and control costs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stop Sequences - This parameter is a string that stops the model from generating tokens. This is another way to control the length and structure of the model&amp;rsquo;s response.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Frequency Penalty - This parameter will add a penalty to the same tokens in the model&amp;rsquo;s output when it appears, which can help to prevent a repetition of this word. Higher values of frequency penalty will reduce the repetition of tokens in the model&amp;rsquo;s output.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Presence Penalty - This parameter will set a penalty to the appeared word (Not add more penalty when the word appears more than once), which can reduce the repetition of the same word in the model&amp;rsquo;s output. Higher values of presence penalty will reduce the repetition of the same word in the model&amp;rsquo;s output.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;prompts&#34;&gt;Prompts&lt;/h2&gt;
&lt;h3 id=&#34;what-is-a-prompt&#34;&gt;What is a prompt?&lt;/h3&gt;
&lt;p&gt;Prompts are used to guide the model to generate the correct outputs. A prompt can contain information like the &lt;strong&gt;instruction&lt;/strong&gt; or &lt;strong&gt;question&lt;/strong&gt; you are passing to the model and include other details such as &lt;strong&gt;context&lt;/strong&gt;, &lt;strong&gt;inputs&lt;/strong&gt;, or &lt;strong&gt;examples&lt;/strong&gt;. You can use these elements to instruct the model more effectively to improve the quality of results.&lt;/p&gt;
&lt;p&gt;When using GPT3.5 or GPT4, the prompt can be structured in three different roles: &lt;strong&gt;system&lt;/strong&gt;, &lt;strong&gt;user&lt;/strong&gt;, and &lt;strong&gt;assistant&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;System - Setting the behavior, character, and background of the models.&lt;/li&gt;
&lt;li&gt;User - Asking the question or providing the context.&lt;/li&gt;
&lt;li&gt;Assistant - Automatically generated by the System and user prompts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Normally we only use user prompts to interact with the model.&lt;/p&gt;
&lt;h3 id=&#34;prompt-format&#34;&gt;Prompt Format&lt;/h3&gt;
&lt;p&gt;The prompt can be formatted with different structures divided into zero-shot or few-shot. Zero-shot prompting means it can directly prompt the model for a response without any examples or demonstrations of the task. One typical zero-shot example is the Question/Answering (QA) format:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Q: &amp;lt;Question&amp;gt;?
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;A: &amp;lt;Answer&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Q: &amp;lt;Question&amp;gt;?
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;A: &amp;lt;Answer&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Q: &amp;lt;Question&amp;gt;?
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;A: 
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Different formats of prompts can be used in various tasks, such as classification prompts:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;This is awesome! // Positive
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;This is bad! // Negative
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Wow, that movie was rad! // Positive
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;What a horrible show! //
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Output:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Negative
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This format is a few-shot that can enable in-context learning.&lt;/p&gt;
&lt;h3 id=&#34;prompt-elements&#34;&gt;Prompt Elements&lt;/h3&gt;
&lt;p&gt;One typical prompt often contains any of the following elements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Instruction&lt;/strong&gt; - a specific task or instruction you want the model to perform&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Context&lt;/strong&gt; - external information or additional context that can steer the model to better responses&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Input Data&lt;/strong&gt; - the input or question that we are interested in finding a response for&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output Indicator&lt;/strong&gt; - the type or format of the output&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;designing-tips&#34;&gt;Designing Tips&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Start Simple&lt;/p&gt;
&lt;p&gt;In the beginning, use simple models and prompts, and iterate continuously to meet the requirements.&lt;/p&gt;
&lt;p&gt;When there is a large task, try to break it down into multiple subtasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Obvious Instruction&lt;/p&gt;
&lt;p&gt;Giving clear instructions when designing the prompt can help to improve the output performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Specificity&lt;/p&gt;
&lt;p&gt;More detailed requirements can help the model output meet the requirements.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Extract the name of places in the following text. 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Desired format:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Place: &amp;lt;comma_separated_list_of_places&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Input: &amp;#34;Although these developments are encouraging to researchers, much is still a mystery. “We often have a black box between the brain and the effect we see in the periphery,” says Henrique Veiga-Fernandes, a neuroimmunologist at the Champalimaud Centre for the Unknown in Lisbon. “If we want to use it in the therapeutic context, we actually need to understand the mechanism.&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Avoid Imprecisseness&lt;/p&gt;
&lt;p&gt;Use the most straightforward language to communicate.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Use 2-3 sentences to explain the concept of prompt engineering to a high school student.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Say Do What&lt;/p&gt;
&lt;p&gt;Avoid saying what not to do but say what to do instead&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>📊Reading Notes for &#34;LEMMA-RCA&#34;</title>
      <link>https://chengaoshen.com/blogs/reading-notes-for-lemma-rca/</link>
      <pubDate>Tue, 09 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/blogs/reading-notes-for-lemma-rca/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;This paper introduces a new large dataset named &lt;strong&gt;LEMMA-RCA&lt;/strong&gt; for diverse RCA tasks across multiple domains and modalities. This dataset contains IT and OT operation systems from the real world. They also evaluate eight baseline methods on this dataset to prove the high quality of LEMMA_RCA. The official website is 
.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;What problem does the paper try to solve？&lt;/p&gt;
&lt;p&gt;The use of automated methods for root cause analysis is crucial, but currently, there is a lack of a mainstream dataset and fair comparison is not possible.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is the proposed solution?&lt;/p&gt;
&lt;p&gt;They proposed a rich dataset LEMMA-RCA containing multiple sub-datasets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What are the key experimental results in this paper?&lt;/p&gt;
&lt;p&gt;Tested the performance of 8 models on the LEMMA-RCA dataset.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What are the main contributions of the paper?&lt;/p&gt;
&lt;p&gt;They propose the LEMMA-RCA dataset and evaluate eight baseline models on this.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What are the strong points and weak points in this paper?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Strong Point: Proposed a new dataset and conducted extensive evaluation.&lt;/li&gt;
&lt;li&gt;Weak Point: There are no baseline methods not belonging to the causal-graph-based model.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;Root cause analysis (RCA) is essential for identifying the underlying causes of system failures and ensuring the reliability and robustness of real-world systems. However, traditional manual RCA is labor-intensive, costly, and prone to errors, so data-driven methods are needed. Despite significant progress in RCA techniques, &lt;strong&gt;the large-scale public datasets remain limited&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In RCA fields, here are some important keywords:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Key Performance Indicator (KPI)&lt;/strong&gt; is a time series indicating the system status, such as latency and service response time in microservice systems.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Entity Metrics&lt;/strong&gt; are multivariate time series collected by monitoring numerous system entities or components, such as CPU/Memory utilization in microservice systems.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data-driven Root Cause Analysis Problem&lt;/strong&gt;. Given the monitoring data of system entities and system KPIs, identify the top K system entities that are relevant to KPIs when the system fails.
&lt;ul&gt;
&lt;li&gt;Offline/Online: Offline RCA only uses historical data to determine past failures; Online RCA operates in real-time using current data streams to promptly address issues.&lt;/li&gt;
&lt;li&gt;Single-modal/multi-modal: Single-modal RCA relies solely on one type of data for a focused analysis; Multi-modal RCA uses multiple data sources for a comprehensive assessment.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/RCA_workflow.png&#34; alt=&#34;RCA workflow&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;RCA workflow&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;h3 id=&#34;base-information&#34;&gt;Base Information&lt;/h3&gt;
&lt;p&gt;LEMMA-RCA is a multi-domain, multi-modal dataset that includes textual system logs with millions of event records and time series metric data collected from real system faults. This dataset includes IT and OT scenes, such as microservice and water treatment.&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/RCA_datasets.png&#34; alt=&#34;Existing datasets for root cause analysis.&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;Existing datasets for root cause analysis.&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;h3 id=&#34;collection&#34;&gt;Collection&lt;/h3&gt;
&lt;p&gt;The dataset collected from two domains, divided into four sub-datasets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;IT operations&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Product Review&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Platform&lt;/strong&gt;: Composed of six OpenShift nodes and 216 system pods.&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20240710000718711.png&#34; alt=&#34;The architecture of Product Review Platform&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;The architecture of Product Review Platform&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Faults&lt;/strong&gt;: out-of-memory, high-CPU-usage, external-storage-full, DDos attack.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Metrics&lt;/strong&gt;: Using Prometheus to record eleven types of node-level metrics and six types of pod-level metrics; Using ElasticSearch to collect log data including timestamp, pod name, log message, etc; Using JMeter to collect the system status information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;KPI&lt;/strong&gt;: Consider latency as system KPI due to system failure will result in latency significantly increasing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cloud Computing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Platform&lt;/strong&gt;: Eleven system nodes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Faults&lt;/strong&gt;: six different types of faults, such as cryptojacking, configuration change failure, etc.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Metrics&lt;/strong&gt;: Extracting system metrics from CloudWatch Metrics on EC2 instances; Extracting three logs types (log messages, API debug log, and MySQL log) from CloudWatch Logs; Using JMeter tools to record error rate and utilization rate as KPIs.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/IT_Sub.png&#34; alt=&#34;Data statistics of IT operation sub-datasets.&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;Data statistics of IT operation sub-datasets.&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OT operations&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SWaT: Collected over an 11-day period from a water treatment testbed equipped with 51 sensors. The system operated normally during the first 7 days, followed by attacks over the last 4 days, resulting in 16 system faults.&lt;/li&gt;
&lt;li&gt;WADI: Gathered from a water distribution testbed over 16 days, featuring 123 sensors and actuators. The system maintained normal operations for the first 14 days before experiencing attacks in the final 2 days, with 15 system faults recorded.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/OT_sub.png&#34; alt=&#34;Data statistics of OT operation sub-datasets.&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;Data statistics of OT operation sub-datasets.&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;h3 id=&#34;preprocessing&#34;&gt;Preprocessing&lt;/h3&gt;
&lt;p&gt;Some non-stationary data are unpredictable and cannot be effectively modeled, which means they should be excluded. Thus this paper introduces some methods to preprocessing the data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Log Feature Extraction&lt;/strong&gt;. Due to the log data being unstructured and some of them being unmeaning,  this paper transforms the log data into the time-series format. First, they use log-parsing tools to structure the log message. Then they segment the data using &lt;em&gt;10-minute windows&lt;/em&gt; with &lt;em&gt;30-second intervals&lt;/em&gt; and calculate the occurrence frequency as the first feature type donated as $X_1^L\in \mathbb{R}^T$. Then, they introduce a second feature type based on &amp;ldquo;Golden signals&amp;rdquo; derived from domain knowledge, such as the frequency of abnormal logs associated with system failures like DDoS attacks, storage failures, and resource over-utilization. This feature is donated as $X_2^L\in \mathbb{R}^T$. Finally, they segment the log using the same time windows and apply PCA to reduce feature dimensionality, selecting the most significant component as $X_3^L\in \mathbb{R}^T$. The overall data can form as matrix $X^L=[X_1^L,X_2^L,X_3^L]\in \mathbb{R}^{3\times T}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;KPI Construction&lt;/strong&gt;. Using anomaly detection algorithms to model the SWaT and WADI datasets, and transform the discrete value into continuous format.&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;metrics&#34;&gt;Metrics&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Precision@K (PR@K)&lt;/strong&gt;: It measures the probability that the top $K$ predicted root causes are real, formulated as:&lt;/p&gt;
$$
\text{PR@K}=\frac{1}{|\mathbb{A}|}\sum_{a\in\mathbb{A}}\frac{\sum_{i\le k}R_a(i)\in V_a}{\text{min}(K,|v_a|)}
$$&lt;p&gt;Where $\mathbb{A}$ is the set of system faults, $a$ is one fault, $V_a$ is the real root cause of $a$, $R_a$ is the predicted root cause of $a$, and $i$ is the $i$-th predicted cause of $R_a$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mean Average Precision@K (MAP@K)&lt;/strong&gt;: It assesses the top $K$ predicted causes from the overall perspective, formulated as:&lt;/p&gt;
$$
\text{MAP@K}=\frac{1}{K|\mathbb{A}|}\sum_{a\in \mathbb{A}}\sum_{i\le j\le K}\text{PR@j}
$$&lt;p&gt;&lt;strong&gt;Mean Reciprocal Rank (MRR)&lt;/strong&gt;: It evaluates the ranking capability of models, formulated as:&lt;/p&gt;
$$
\text{MRR@K}=\frac{1}{|\mathbb{A}|}\sum_{a\in \mathbb{A}}\frac{1}{\text{rank}_{R_a}}
$$&lt;p&gt;Where $\text{rank}_{R_{a}}$ is the rank number of the first correctly predicted root cause for system fault $a$.&lt;/p&gt;
&lt;h3 id=&#34;baselines&#34;&gt;Baselines&lt;/h3&gt;
&lt;p&gt;Causal-graph-based RCA methods can provide deeper insights into system failures, thus all baseline methods fall into this category.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PC: Classic constrain-based causal discovery algorithm that can identify the causal graph&amp;rsquo;s skeleton using an independence test.&lt;/li&gt;
&lt;li&gt;Dynotears: It construct dynamic Bayesian networks through vector autoregression models.&lt;/li&gt;
&lt;li&gt;C-LSTM: Utilizes LSTM to model temporal dependencies and capture nonlinear Granger causality.&lt;/li&gt;
&lt;li&gt;GOLEM:  relaxing the hard Directed Acyclic Graph (DAG) constraint of NOTEARS with a scoring function&lt;/li&gt;
&lt;li&gt;REASON: An interdependent network model learning both intra-level and inter-level causal relationships.&lt;/li&gt;
&lt;li&gt;Nezha: A multi-modal method designed to identify root causes by detecting abnormal patterns.&lt;/li&gt;
&lt;li&gt;MULAN: A multi-modal RCA method that learns the correlation between different modalities and co-constructs a causal graph for root cause identification&lt;/li&gt;
&lt;li&gt;CORAL: An online single-modal RCA method based on incremental disentangled causal graph learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/RCA_Product.png&#34; alt=&#34;Results for offline RCA baselines with multiple modalities on the Product Review dataset.&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;Results for offline RCA baselines with multiple modalities on the Product Review dataset.&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/RCA_SAT.png&#34; alt=&#34;Results for offline RCA baselines on the SWaT and WADI dataset.&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;Results for offline RCA baselines on the SWaT and WADI dataset.&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/RCA.png&#34; alt=&#34;Results for online root cause analysis baselines on all sub-datasets.&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;Results for online root cause analysis baselines on all sub-datasets.&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>📑Reading Notes for &#34;LTSF-Linear&#34;</title>
      <link>https://chengaoshen.com/blogs/reading-notes-for-ltsf-linear/</link>
      <pubDate>Sat, 22 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/blogs/reading-notes-for-ltsf-linear/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;This paper insists that the time series is an ordered set of continuous points that will result in the loss of temporal information when using the Transformer structure. To prove this opinion, they propose models named &lt;strong&gt;LSTF-Linear&lt;/strong&gt; which achieve outstanding performance and conduct comprehensive studies.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;What problem does the paper try to solve？&lt;/p&gt;
&lt;p&gt;Attempt to verify whether the Transformer is effective in time series prediction problems and whether a simple model can surpass all current Transformer-based models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is the proposed solution?&lt;/p&gt;
&lt;p&gt;The LSTF Linear model was proposed as an alternative, achieving extremely high performance with only a single-layer network&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What are the key experimental results in this paper?&lt;/p&gt;
&lt;p&gt;Achieved better performances than Transformer-based methods on multiple datasets such as electricity, healthcare, and meteorology&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What are the main contributions of the paper?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Challenge the Transformer structure in the long-term time series forecasting task.&lt;/li&gt;
&lt;li&gt;Introduce the LTSF-Linear model which only has one layer while achieving compared results in various fields.&lt;/li&gt;
&lt;li&gt;Conduct comprehensive empirical studies on various aspects of existing Transformer-based solutions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What are the strong points and weak points in this paper?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Strong Points&lt;/strong&gt;： Proposed potential issues in the current research route and opened up a new perspective in a simple way.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Weak Points&lt;/strong&gt;: Only conducted research on prediction problems and have not explored other issues, such as anomaly detection.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;Over the past several decades, the Transformer has been widely used as the TSF solution. However, the self-attention mechanism is &lt;strong&gt;permutation-invariant&lt;/strong&gt; and &lt;strong&gt;anti-order&lt;/strong&gt; which will cause the loss of temporal information. Typically, time series contain less semantic meaning compared with NLP or CV problems and need more temporal information, which will emphasize this problem. Thus, this paper tries to challenge the Transformer-based LSTF solution with direct multi-step forecasting strategies.&lt;/p&gt;
&lt;p&gt;For the time series containing $C$ variates, the historical data can be represented as $\mathcal{X}=\{X_1^t,\dots,X_C^t\}_{t=1}^L$, wherein $L$ is the look-back window size. The forecasting problems need to predict $T$ feature time step’s value $\hat{\mathcal{X}}=\{\hat{X}_1^t,\dots,\hat{X}_C^t\}_{t=1}^L$. When $T&gt;1$, the methods can be divided into two parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Integrated multi-step(IMS): Learn a single-step forecaster and interactively apply it to obtain multi-step predictions. This method has a smaller variance but will cause error accumulation.&lt;/li&gt;
&lt;li&gt;Direct multi-step(DMS): Directly optimize the multi-step forecasting objective at one. This method can have more accurate predictions when $T$ is large.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;methods&#34;&gt;Methods&lt;/h2&gt;
&lt;h3 id=&#34;transformer-based-methods&#34;&gt;Transformer-Based Methods&lt;/h3&gt;
&lt;p&gt;The vanilla Transformer model has some limitations when applied to time series problems, thus various works try to improve the performance by adding or replacing some parts of the Transformer. Generally speaking, it can be divided into four major parts.&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20240622212351839.png&#34; alt=&#34;The pipeline of Transformer-Based Methods&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;The pipeline of Transformer-Based Methods&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Preprocessing&lt;/strong&gt;: To use Transformer to deal with time series datasets, some preprocessing is needed to adopt the data structures, such as normalization with zero-mean and adding timestamps as NLP did. Specifically, in Autoformer, seasonal-trend decomposition is proposed to get the trend part and the cyclical part, which helps data more clearly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Embedding&lt;/strong&gt;: In NLP’s Transformer, embedding will map the words to the vector in a typical space that reveals the meaning. In time series, time information is significantly important. Thus, various timestamp methods are proposed to help the model reserve the temporal information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Encoder/Decoder&lt;/strong&gt;: To help the Transformer structure adopt the time series problems, there have been many improvements made to the encoder and decoder in this work. Among them, in the encoder section, many improvements have been proposed to reduce computational consumption and increase speed. In the decoder section, to avoid cumulative errors, it has also begun to transition from IMS to DMS.&lt;/p&gt;
&lt;p&gt;The success of Transformer in the NLP field is largely attributed to its understanding of the semantic relationships between words, but in time series problems, temporary information has become even more important. However, the Transformer&amp;rsquo;s ability to model time largely comes from the timestamp rather than its structure.&lt;/p&gt;
&lt;h3 id=&#34;lstf-linear&#34;&gt;LSTF-Linear&lt;/h3&gt;
&lt;p&gt;This paper hypothesizes that the improvement in the Transformer-Based model is due to the DMS strategy rather than the Transformer. Thus they propose LSTF-Linear that directly regresses historical time series for future prediction via a weighted sum operation to verify this thinking. The model can be a formula as :
&lt;/p&gt;
$$
\hat{X}_i=WX_i
$$&lt;p&gt;
Wherein $W\in\mathbb{R}^{T\times L}$ is a linear layer along the temporal axis. Furthermore, this paper proposes two sub-models, DLinear which uses decomposition to obtain the trend part and the seasonal part, and the NLinear which subtracts a portion before making the prediction and rejoined it after making the prediction.&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20240622212512231.png&#34; alt=&#34;The Structure of LSTF-Linear&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;The Structure of LSTF-Linear&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;In order to verify the quality of the LSTF-Linear, the author selected some common sequence data from real life and compared with five popular Transformer-based model. All results are show as follow:&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20240623194839137.png&#34; alt=&#34;Datasets Description&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;Datasets Description&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20240623194933447.png&#34; alt=&#34;Model Comparison&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;Model Comparison&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20240623195006823.png&#34; alt=&#34;Visualize results&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;Visualize results&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20240623200500165.png&#34; alt=&#34;MSE in different look-back windows size&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;MSE in different look-back windows size&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>🚀 A Brand New Personal Homepage!</title>
      <link>https://chengaoshen.com/blogs/hello/</link>
      <pubDate>Thu, 06 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/blogs/hello/</guid>
      <description>&lt;p&gt;In the upcoming Fall of 2024, I will be completing my undergraduate studies and beginning my Ph.D. journey. To better present my academic experience and personal information, I have used Hugo to create a new personal homepage (which is the page you are currently viewing)!&lt;/p&gt;
&lt;p&gt;I will continue to update my academic blog and personal news on this homepage. Stay tuned!&lt;/p&gt;
&lt;p&gt;More information about me can be found in 
. I sincerely hope my posts can help you. If you have any questions, please feel free to contact me.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Emoji kitchen with controlled fusion</title>
      <link>https://chengaoshen.com/publication/emoji-kitchen-with-controlled-fusion/</link>
      <pubDate>Fri, 31 May 2024 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/publication/emoji-kitchen-with-controlled-fusion/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Portable, intelligent MIECL sensing platform for ciprofloxacin detection using a fast convolutional neural networks-assisted Tb@Lu2O3 nanoemitter</title>
      <link>https://chengaoshen.com/publication/sensing-platform/</link>
      <pubDate>Fri, 31 May 2024 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/publication/sensing-platform/</guid>
      <description></description>
    </item>
    
    <item>
      <title>🤗Introduction to Generative Models</title>
      <link>https://chengaoshen.com/blogs/introduction-to-generative-models/</link>
      <pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/blogs/introduction-to-generative-models/</guid>
      <description>&lt;p&gt;Generative Models are part of unsupervised learning models that can learned from the datasets without any labels. Unlike other unsupervised models to manipulate, denoise, interpolate between, or compress examples, generative models focus on generating plausible new samples having similar properties to the dataset.&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20231025211322464.png&#34; alt=&#34;Taxonomy of unsupervised learning models&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;Taxonomy of unsupervised learning models&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Latent variable models&lt;/strong&gt;: mapping the data examples $\mathbf{x}$ to unseen latent variables $\mathbf{z}$ which can capture the underlying structure in the dataset.&lt;/p&gt;
&lt;p&gt;In this essay, we will introduce the categories of generative models, discuss their properties, and talk about how to measure them.&lt;/p&gt;
&lt;h2 id=&#34;what-are-probabilistic-models&#34;&gt;What are probabilistic models?&lt;/h2&gt;
&lt;p&gt;Before we dive into the typical forms of generative models, we should understand two major categories of them.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Direct generative models, such as generative adversarial models, aim to provide a mechanism for generating samples similar to observed data $\{\mathbf{x}_i\}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Probabilistic models learn the probability distribution over the train data. These models will assign a probability $Pr(\mathbf{x}|\phi)$ to each data point $\mathbf{x}$. The models aim to maximize the probability of the observed data $\{\mathbf{x}_i\}$.
&lt;/p&gt;
$$
  L[\phi]=-\sum_{i=1}^{I}\mathrm{log}[Pr(\mathbf{x}_i|\phi)]
  $$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is an image to describe the training process of the two models.&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/Training%20of%20generative%20models.png&#34; alt=&#34;Training of generative models&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;Training of generative models&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&#34;properties-of-generative-models&#34;&gt;Properties of Generative Models&lt;/h2&gt;
&lt;p&gt;There are several properties that Generative Models need to have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Efficient sampling: using less computational consumption.&lt;/li&gt;
&lt;li&gt;High-quality sampling: output is indistinguishable from train data&lt;/li&gt;
&lt;li&gt;Coverage: samples should represent the entire training distribution&lt;/li&gt;
&lt;li&gt;Well-behaved latent space: change in latent space will perform in data similarly.&lt;/li&gt;
&lt;li&gt;Efficient likelihood computation: able to calculate the probability of new examples efficiently.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It’s hard for only one type of Generative Model to obtain all properties, following is a table to describe some generative model’s features:&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;Model&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;Probabilistic&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;Efficient&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;Sample quality&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;Coverage&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;Well-behaved latent space&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;Disentangled latent space&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;Efficient likelihood&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;strong&gt;GANs&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✖️&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✔️&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✔️&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✖️&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✔️&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;❔&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;\&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;strong&gt;Flows&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✔️&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✔️&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✖️&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;❔&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✔️&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;❔&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✖️&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;strong&gt;VAEs&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✔️&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✔️&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✖️&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;❔&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✔️&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;❔&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✔️&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;strong&gt;Diffusion&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✔️&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✖️&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✔️&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;❔&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✖️&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✖️&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;✖️&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;performance-measurement&#34;&gt;Performance Measurement&lt;/h2&gt;
&lt;p&gt;To measure the performance of generative models, some metrics are proposed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inception score (IS)&lt;/strong&gt;: This metric is used in image generative models which trained on the ImageNet dataset. There are two criteria to design it. First, each generated image $\mathbf{x}^*$ should look like only one class in the ImageNet dataset which has 1000 possible classes. Second, the probability for each class in generated images should be equal.&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20231026144638161.png&#34; alt=&#34;Inception score&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;Inception score&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;This metric uses KL divergence between $Pr(y_i|\mathbf{x}_i^*)$and $Pr(y)$
&lt;/p&gt;
$$
IS=\mathrm{exp}\bigg[\frac{1}{I}\sum_{i=1}^{I}D_{KL}\big[Pr(y_i|\mathbf{x}_i^*)||Pr(y)\big]\bigg]
$$&lt;p&gt;
Where $I$ is the number of generated images and:
&lt;/p&gt;
$$
Pr(y)=\frac{1}{I}\sum_{i=1}^{I}Pr(y_i|\mathbf{x}_i^*)
$$&lt;p&gt;
&lt;strong&gt;Fréchet inception distance&lt;/strong&gt;: To decrease the reliance on the ImageNet dataset and characterize either distribution, the Fréchet inception distance is estimated.&lt;/p&gt;
&lt;p&gt;First, use the inception model accepting both observed and generated images as input to produce $1\times 2048$ feature vector. And, because the images follow the normal distribution which can be defined by mean and variance, we can use them to calculate the distance between real and generated images.&lt;/p&gt;
&lt;p&gt;Above all, the formula for calculating FID can be written as follows:
&lt;/p&gt;
$$
FID(x,g)=||\mu_x-\mu_g||_2^2+Tr\big(\Sigma_x+\Sigma_g-2(\Sigma_x\Sigma_g)^{0.5}\big)
$$&lt;p&gt;
Where $x,g$ present real and generated images, $\mu$ is the mean and $\Sigma$ is the covariance.&lt;/p&gt;
&lt;p&gt;However, this metric uses the inception network output to calculate, which will more focus on the semantic information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Manifold precision/recall&lt;/strong&gt;: To disentangle the realism of the samples and their diversity, we consider the overlap between the data manifold and the model manifold.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Precision is the fraction of &lt;strong&gt;model samples&lt;/strong&gt; that fall into the &lt;strong&gt;data manifold&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Recall is the fraction of &lt;strong&gt;data examples&lt;/strong&gt; that fall within the &lt;strong&gt;model manifold&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, ‘Rethinking the Inception Architecture for Computer Vision’, in &lt;em&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, 2016.&lt;/p&gt;
&lt;p&gt;[2] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, ‘GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium’, in &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, 2017, vol. 30.&lt;/p&gt;
&lt;p&gt;[3] S. J. D. Prince, &lt;em&gt;Understanding Deep Learning&lt;/em&gt;. MIT Press, 2023.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>📜Notes on LaTeX formulas</title>
      <link>https://chengaoshen.com/blogs/various-font-in-latex/</link>
      <pubDate>Wed, 25 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/blogs/various-font-in-latex/</guid>
      <description>&lt;p&gt;$\LaTeX$ is a document preparation system used for the communication and publication of scientific documents. However, it&amp;rsquo;s not easy to use this to write formulas, especially for the beginners, typically how to choose the right font and Greek letter.&lt;/p&gt;
&lt;p&gt;This essay is a note for me to remember the latex formulas, which have vairous fonts and Greek letter. Furthermore I also record some special symbols that can be used in the Future papers.&lt;/p&gt;
&lt;h2 id=&#34;various-fonts&#34;&gt;Various Fonts&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Normal form
&lt;ul&gt;
&lt;li&gt;$A$, $B$, $C$, $D$, $E$, $F$, $G$, $H$, $I$, $J$, $K$, $L$, $M$, $N$, $O$, $P$, $Q$, $R$, $S$, $T$, $U$, $V$, $W$, $X$, $Y$, $Z$&lt;/li&gt;
&lt;li&gt;$a$, $b$, $c$, $d$, $e$, $f$, $g$, $h$, $i$, $j$, $k$, $l$, $m$, $n$, $o$, $p$, $q$, $r$, $s$, $t$, $u$, $v$, $w$, $x$, $y$, $z$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Boldface: &lt;code&gt;\mathbf{}&lt;/code&gt;.
&lt;ul&gt;
&lt;li&gt;$\mathbf{A}$, $\mathbf{B}$, $\mathbf{C}$, $\mathbf{D}$, $\mathbf{E}$, $\mathbf{F}$, $\mathbf{G}$, $\mathbf{H}$, $\mathbf{I}$, $\mathbf{J}$, $\mathbf{K}$, $\mathbf{L}$, $\mathbf{M}$, $\mathbf{N}$, $\mathbf{O}$, $\mathbf{P}$, $\mathbf{Q}$, $\mathbf{R}$, $\mathbf{S}$, $\mathbf{T}$, $\mathbf{U}$, $\mathbf{V}$, $\mathbf{W}$, $\mathbf{X}$, $\mathbf{Y}$, $\mathbf{Z}$&lt;/li&gt;
&lt;li&gt;$\mathbf{a}$, $\mathbf{b}$, $\mathbf{c}$, $\mathbf{d}$, $\mathbf{e}$, $\mathbf{f}$, $\mathbf{g}$, $\mathbf{h}$, $\mathbf{i}$, $\mathbf{j}$, $\mathbf{k}$, $\mathbf{l}$, $\mathbf{m}$, $\mathbf{n}$, $\mathbf{o}$, $\mathbf{p}$, $\mathbf{q}$, $\mathbf{r}$, $\mathbf{s}$, $\mathbf{t}$, $\mathbf{u}$, $\mathbf{v}$, $\mathbf{w}$, $\mathbf{x}$, $\mathbf{y}$, $\mathbf{z}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Calligraphic font: &lt;code&gt;\mathcal{}&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;$\mathcal{A}$, $\mathcal{B}$, $\mathcal{C}$, $\mathcal{D}$, $\mathcal{E}$, $\mathcal{F}$, $\mathcal{G}$, $\mathcal{H}$, $\mathcal{I}$, $\mathcal{J}$, $\mathcal{K}$, $\mathcal{L}$, $\mathcal{M}$, $\mathcal{N}$, $\mathcal{O}$, $\mathcal{P}$, $\mathcal{Q}$, $\mathcal{R}$, $\mathcal{S}$, $\mathcal{T}$, $\mathcal{U}$, $\mathcal{V}$, $\mathcal{W}$, $\mathcal{X}$, $\mathcal{Y}$, $\mathcal{Z}$&lt;/li&gt;
&lt;li&gt;$\mathcal{a}$, $\mathcal{b}$, $\mathcal{c}$, $\mathcal{d}$, $\mathcal{e}$, $\mathcal{f}$, $\mathcal{g}$, $\mathcal{h}$, $\mathcal{i}$, $\mathcal{j}$, $\mathcal{k}$, $\mathcal{l}$, $\mathcal{m}$, $\mathcal{n}$, $\mathcal{o}$, $\mathcal{p}$, $\mathcal{q}$, $\mathcal{r}$, $\mathcal{s}$, $\mathcal{t}$, $\mathcal{u}$, $\mathcal{v}$, $\mathcal{w}$, $\mathcal{x}$, $\mathcal{y}$, $\mathcal{z}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Roman font: &lt;code&gt;\mathrm{}&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;$\mathrm{A}$, $\mathrm{B}$, $\mathrm{C}$, $\mathrm{D}$, $\mathrm{E}$, $\mathrm{F}$, $\mathrm{G}$, $\mathrm{H}$, $\mathrm{I}$, $\mathrm{J}$, $\mathrm{K}$, $\mathrm{L}$, $\mathrm{M}$, $\mathrm{N}$, $\mathrm{O}$, $\mathrm{P}$, $\mathrm{Q}$, $\mathrm{R}$, $\mathrm{S}$, $\mathrm{T}$, $\mathrm{U}$, $\mathrm{V}$, $\mathrm{W}$, $\mathrm{X}$, $\mathrm{Y}$, $\mathrm{Z}$&lt;/li&gt;
&lt;li&gt;$\mathrm{a}$, $\mathrm{b}$, $\mathrm{c}$, $\mathrm{d}$, $\mathrm{e}$, $\mathrm{f}$, $\mathrm{g}$, $\mathrm{h}$, $\mathrm{i}$, $\mathrm{j}$, $\mathrm{k}$, $\mathrm{l}$, $\mathrm{m}$, $\mathrm{n}$, $\mathrm{o}$, $\mathrm{p}$, $\mathrm{q}$, $\mathrm{r}$, $\mathrm{s}$, $\mathrm{t}$, $\mathrm{u}$, $\mathrm{v}$, $\mathrm{w}$, $\mathrm{x}$, $\mathrm{y}$, $\mathrm{z}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sans-serif font: &lt;code&gt;\mathsf{}&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;$\mathsf{A}$, $\mathsf{B}$, $\mathsf{C}$, $\mathsf{D}$, $\mathsf{E}$, $\mathsf{F}$, $\mathsf{G}$, $\mathsf{H}$, $\mathsf{I}$, $\mathsf{J}$, $\mathsf{K}$, $\mathsf{L}$, $\mathsf{M}$, $\mathsf{N}$, $\mathsf{O}$, $\mathsf{P}$, $\mathsf{Q}$, $\mathsf{R}$, $\mathsf{S}$, $\mathsf{T}$, $\mathsf{U}$, $\mathsf{V}$, $\mathsf{W}$, $\mathsf{X}$, $\mathsf{Y}$, $\mathsf{Z}$&lt;/li&gt;
&lt;li&gt;$\mathsf{a}$, $\mathsf{b}$, $\mathsf{c}$, $\mathsf{d}$, $\mathsf{e}$, $\mathsf{f}$, $\mathsf{g}$, $\mathsf{h}$, $\mathsf{i}$, $\mathsf{j}$, $\mathsf{k}$, $\mathsf{l}$, $\mathsf{m}$, $\mathsf{n}$, $\mathsf{o}$, $\mathsf{p}$, $\mathsf{q}$, $\mathsf{r}$, $\mathsf{s}$, $\mathsf{t}$, $\mathsf{u}$, $\mathsf{v}$, $\mathsf{w}$, $\mathsf{x}$, $\mathsf{y}$, $\mathsf{z}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Typewriter font: &lt;code&gt;\mathtt{}&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;$\mathtt{A}$, $\mathtt{B}$, $\mathtt{C}$, $\mathtt{D}$, $\mathtt{E}$, $\mathtt{F}$, $\mathtt{G}$, $\mathtt{H}$, $\mathtt{I}$, $\mathtt{J}$, $\mathtt{K}$, $\mathtt{L}$, $\mathtt{M}$, $\mathtt{N}$, $\mathtt{O}$, $\mathtt{P}$, $\mathtt{Q}$, $\mathtt{R}$, $\mathtt{S}$, $\mathtt{T}$, $\mathtt{U}$, $\mathtt{V}$, $\mathtt{W}$, $\mathtt{X}$, $\mathtt{Y}$, $\mathtt{Z}$&lt;/li&gt;
&lt;li&gt;$\mathtt{a}$, $\mathtt{b}$, $\mathtt{c}$, $\mathtt{d}$, $\mathtt{e}$, $\mathtt{f}$, $\mathtt{g}$, $\mathtt{h}$, $\mathtt{i}$, $\mathtt{j}$, $\mathtt{k}$, $\mathtt{l}$, $\mathtt{m}$, $\mathtt{n}$, $\mathtt{o}$, $\mathtt{p}$, $\mathtt{q}$, $\mathtt{r}$, $\mathtt{s}$, $\mathtt{t}$, $\mathtt{u}$, $\mathtt{v}$, $\mathtt{w}$, $\mathtt{x}$, $\mathtt{y}$, $\mathtt{z}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Blackboard bold: &lt;code&gt;\mathbb{}&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbb{A}$, $\mathbb{B}$, $\mathbb{C}$, $\mathbb{D}$, $\mathbb{E}$, $\mathbb{F}$, $\mathbb{G}$, $\mathbb{H}$, $\mathbb{I}$, $\mathbb{J}$, $\mathbb{K}$, $\mathbb{L}$, $\mathbb{M}$, $\mathbb{N}$, $\mathbb{O}$, $\mathbb{P}$, $\mathbb{Q}$, $\mathbb{R}$, $\mathbb{S}$, $\mathbb{T}$, $\mathbb{U}$, $\mathbb{V}$, $\mathbb{W}$, $\mathbb{X}$, $\mathbb{Y}$, $\mathbb{Z}$&lt;/li&gt;
&lt;li&gt;$\mathbb{a}$, $\mathbb{b}$, $\mathbb{c}$, $\mathbb{d}$, $\mathbb{e}$, $\mathbb{f}$, $\mathbb{g}$, $\mathbb{h}$, $\mathbb{i}$, $\mathbb{j}$, $\mathbb{k}$, $\mathbb{l}$, $\mathbb{m}$, $\mathbb{n}$, $\mathbb{o}$, $\mathbb{p}$, $\mathbb{q}$, $\mathbb{r}$, $\mathbb{s}$, $\mathbb{t}$, $\mathbb{u}$, $\mathbb{v}$, $\mathbb{w}$, $\mathbb{x}$, $\mathbb{y}$, $\mathbb{z}$,&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Script font: &lt;code&gt;\mathscr{}&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;$\mathscr{A}$, $\mathscr{B}$, $\mathscr{C}$, $\mathscr{D}$, $\mathscr{E}$, $\mathscr{F}$, $\mathscr{G}$, $\mathscr{H}$, $\mathscr{I}$, $\mathscr{J}$, $\mathscr{K}$, $\mathscr{L}$, $\mathscr{M}$, $\mathscr{N}$, $\mathscr{O}$, $\mathscr{P}$, $\mathscr{Q}$, $\mathscr{R}$, $\mathscr{S}$, $\mathscr{T}$, $\mathscr{U}$, $\mathscr{V}$, $\mathscr{W}$, $\mathscr{X}$, $\mathscr{Y}$, $\mathscr{Z}$&lt;/li&gt;
&lt;li&gt;$\mathscr{a}$, $\mathscr{b}$, $\mathscr{c}$, $\mathscr{d}$, $\mathscr{e}$, $\mathscr{f}$, $\mathscr{g}$, $\mathscr{h}$, $\mathscr{i}$, $\mathscr{j}$, $\mathscr{k}$, $\mathscr{l}$, $\mathscr{m}$, $\mathscr{n}$, $\mathscr{o}$, $\mathscr{p}$, $\mathscr{q}$, $\mathscr{r}$, $\mathscr{s}$, $\mathscr{t}$, $\mathscr{u}$, $\mathscr{v}$, $\mathscr{w}$, $\mathscr{x}$, $\mathscr{y}$, $\mathscr{z}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fraktur font: &lt;code&gt;\mathfrak{}&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;$\mathfrak{A}$, $\mathfrak{B}$, $\mathfrak{C}$, $\mathfrak{D}$, $\mathfrak{E}$, $\mathfrak{F}$, $\mathfrak{G}$, $\mathfrak{H}$, $\mathfrak{I}$, $\mathfrak{J}$, $\mathfrak{K}$, $\mathfrak{L}$, $\mathfrak{M}$, $\mathfrak{N}$, $\mathfrak{O}$, $\mathfrak{P}$, $\mathfrak{Q}$, $\mathfrak{R}$, $\mathfrak{S}$, $\mathfrak{T}$, $\mathfrak{U}$, $\mathfrak{V}$, $\mathfrak{W}$, $\mathfrak{X}$, $\mathfrak{Y}$, $\mathfrak{Z}$&lt;/li&gt;
&lt;li&gt;$\mathfrak{a}$, $\mathfrak{b}$, $\mathfrak{c}$, $\mathfrak{d}$, $\mathfrak{e}$, $\mathfrak{f}$, $\mathfrak{g}$, $\mathfrak{h}$, $\mathfrak{i}$, $\mathfrak{j}$, $\mathfrak{k}$, $\mathfrak{l}$, $\mathfrak{m}$, $\mathfrak{n}$, $\mathfrak{o}$, $\mathfrak{p}$, $\mathfrak{q}$, $\mathfrak{r}$, $\mathfrak{s}$, $\mathfrak{t}$, $\mathfrak{u}$, $\mathfrak{v}$, $\mathfrak{w}$, $\mathfrak{x}$, $\mathfrak{y}$, $\mathfrak{z}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;typical-example&#34;&gt;Typical Example&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Vector：$\mathbf{x}$&lt;/li&gt;
&lt;li&gt;Matrix: $\mathbf{M}$&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>💡Introduction to Transformer</title>
      <link>https://chengaoshen.com/blogs/introduction-to-transformer/</link>
      <pubDate>Mon, 23 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/blogs/introduction-to-transformer/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Transformer is a really popular method in modern neural networks. We have BERT or GPT to process the natural language and ViT to deal with computer vision. In this essay, you will understand what is the transformer and why the transformer works. But be careful, limited by my knowledge, I can’t show some mathematical theories or code of transformer for you.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;why-do-we-need-the-transformer&#34;&gt;Why do we need the Transformer?&lt;/h2&gt;
&lt;p&gt;In the NLP( Natural Language Processing) field, the text dataset always has some obvious features that prevent us from using MLP.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Too large after being encoder.&lt;/p&gt;
&lt;p&gt;To represent the words, we should embed them into vectors first. Generally, we use the vector of length 1024 to describe one word, meaning the data size will become large after multiplying the size of the vector(1024).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Having different length&lt;/p&gt;
&lt;p&gt;Our inputs for the NLP problems have various lengths according to the size of passages or sentences. We should build models that adopt different size inputs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ambiguous&lt;/p&gt;
&lt;p&gt;The texts don&amp;rsquo;t resemble the numbers having certain meanings from themselves. Some words like “it” refer to others in the context, which means they are ambiguous in different backgrounds.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To sum up, the features or problems we mention above prevent us from using normal structures like MLP to process natural language. So we need new things - Transformer. Let us approach this by self-attention first.&lt;/p&gt;
&lt;h2 id=&#34;self-attention&#34;&gt;Self-attention&lt;/h2&gt;
&lt;h3 id=&#34;parameter-sharing&#34;&gt;Parameter sharing&lt;/h3&gt;
&lt;p&gt;A standard neural network layer $f[x]$, take $D\times 1$ inputs and return $D&#39;\times 1$. When we process embedding text, our input’s dimensions are changed to $D\times N$. To hold enough information in our model, we assume the size of our data remains unchanged, which means the size of outputs is also $D\times N$. Where $D$ is the size of the embedding vector, $N$ is the number of the word.&lt;/p&gt;
&lt;p&gt;Roughly, different words have their meanings just like the dictionary shows, which inspires us to use the neural network to approximate the dictionary - mapping the words to their abstract meanings described by numbers.
&lt;/p&gt;
$$
v=\beta_v+\Omega_vx
$$&lt;p&gt;Where $x$ is embedding word, $v$ is abstract meaning of word(present by number), and $\beta_v,\Omega_v$ is the parameter mapping the words to their meanings.&lt;/p&gt;
&lt;p&gt;As we can use one dictionary to map every word to their meanings, we can just use $\beta_v,\Omega_v$ to map every embedding word, called &lt;strong&gt;parameter sharing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Normally, the inputs and outputs are represented as $D\times N$ matrix, so we can rewrite our equation as follows
&lt;/p&gt;
$$
V[X]=\beta_v1^T+\Omega_vX
$$&lt;h3 id=&#34;from-weight-to-self-attention&#34;&gt;From weight to self-attention&lt;/h3&gt;
&lt;p&gt;In the last section, we use “dictionary” to map words, but the problem is we haven’t considered the context of the words. So, how can we let our “dictionary” know the context?
One simple thinking is we can give every “meaning” a &lt;strong&gt;weight&lt;/strong&gt; and weighted sum the values.
&lt;/p&gt;
$$
O[X]=V[X]\times W
$$$$
W=\begin{pmatrix}
 w_1 &amp;\dots  &amp;w_n \\
 \vdots  &amp;  &amp;\vdots  \\
 w_1 &amp; \dots &amp;w_n
\end{pmatrix}
$$&lt;p&gt;How can we get this weight that measures the relationship between the words in the sentence? Inspired by the search system, we can use “queries” to match “key”. Same as we did before, we can just use linear transformation to get them.
&lt;/p&gt;
$$
\begin{split}
Q[X]=\beta_q1^T+\Omega_qX\\
K[X]=\beta_k1^T+\Omega_kX
\end{split}
$$&lt;p&gt;Where $\beta_q,\Omega_q,\beta_k,\Omega_q$ are also shared for every word.&lt;/p&gt;
&lt;p&gt;Then, we can use $K$ to query the $Q$, telling us which word should pay how much &lt;strong&gt;attention&lt;/strong&gt; to others. Typically, the attention should sum up as $1$, so we can use the softmax function to achieve this. Because we use $X$ to pay attention to itself, we call this &lt;strong&gt;Self-attention&lt;/strong&gt;. To sum up, our model is described as follows:
&lt;/p&gt;
$$
Sa[X]=V[X]\cdot \text{Softmax}[K[X]^TQ[X]]
$$&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20230726112139933.png&#34; alt=&#34;Self-attention&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;Self-attention&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Typically, the dot products will be really large, which means the small changes to the inputs have nearly no effect on the output because we use the softmax function. So we always use the dimension of the input, always the shape of embedding, to scale the dot products.&lt;/p&gt;
$$
Sa[X]=V[X]\cdot \text{Softmax}[\frac{K[X]^TQ[X]}{\sqrt{D_q}}]
$$&lt;h3 id=&#34;multiple-heads&#34;&gt;Multiple heads&lt;/h3&gt;
&lt;p&gt;To increase the information that model learning, we always use more attention units when we calculate the output. We will divide the input to $h$ parts, and calculate the self-attention with them. After that, we will combine all of them to recreate our output.
&lt;/p&gt;
$$
\text{MhSa}[X]=\Omega_c[Sa_1[X]^T,Sa_2[X]^T,\dots,Sa_H[X]^T]^T
$$&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20230726201710681.png&#34; alt=&#34;Multiple heads&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;Multiple heads&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&#34;transformer-layers&#34;&gt;Transformer layers&lt;/h2&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20230726113937045.png&#34; alt=&#34;Transformer layers&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;Transformer layers&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;With &lt;strong&gt;self-attention&lt;/strong&gt; and &lt;strong&gt;MLP&lt;/strong&gt;, we can construct the transformer layers applied in many outstanding models. The process looks like this:
&lt;/p&gt;
$$
\begin{split}
X \leftarrow X+\text{MhSa}[X]\\
X\leftarrow \text{LayerNorm}[X]\\
x_n\leftarrow x_n+\text{MLP}[x_n]\\
X\leftarrow \text{LayerNorm}[X]
\end{split}
$$&lt;p&gt;Where $\text{MLP}$ is fully connected network works separately on each word and $\text{LayerNorm}$ is the normalization happens in the channel, like:&lt;/p&gt;
$$
y=\frac{x-E[x]}{\sqrt{Var[x]+\epsilon }}*\gamma+\beta
$$&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] S. J. D. Prince, &lt;em&gt;Understanding Deep Learning&lt;/em&gt;. MIT Press, 2023.&lt;/p&gt;
&lt;p&gt;[2] A. Vaswani &lt;em&gt;et al.&lt;/em&gt;, ‘Attention Is All You Need’, &lt;em&gt;CoRR&lt;/em&gt;, vol. abs/1706.03762, 2017.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;All photos in this article are from 
. It’s a great book and I really recommend it&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>⚕️Brief introduction of the Tensor in PyTorch</title>
      <link>https://chengaoshen.com/blogs/brief-introduction-of-the-tensor-in-pytorch/</link>
      <pubDate>Wed, 26 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/blogs/brief-introduction-of-the-tensor-in-pytorch/</guid>
      <description>&lt;p&gt;Tensor is a specialized data structure that is very similar to arrays and matrices. We can use it to encode the input and output of the model. Tensors can run on GPUs and other hardware.&lt;/p&gt;
&lt;h2 id=&#34;initializing-a-tensors&#34;&gt;Initializing a Tensors&lt;/h2&gt;
&lt;p&gt;Tensors can be initialized in various ways,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Import the library&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Directly from data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# From Numpy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;np_data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;t_np&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from_numpy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;np_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# From other tensors&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# In this way, it will retains same properties&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;t_ones&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ones_like&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# override the datatype&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;t_random&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rand_like&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;float&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# With static shape&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;rand_t&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rand&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;ones_t&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ones&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;zeros_t&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Careful&lt;/strong&gt;: If you initialize the tensors from Numpy, they will share the same underlying memory, which means that if changing the numpy array, the tensor will change too.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;attributes-of-a-tensor&#34;&gt;Attributes of a Tensor&lt;/h2&gt;
&lt;p&gt;We can get some attributes of a tensor by code followed&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Get the shape of the tensor&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Get the dtype&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#  return the device(CPU or GPU) for this tensor&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;operation-on-tensor&#34;&gt;Operation on Tensor&lt;/h2&gt;
&lt;p&gt;There are over 100 operations for tensors, including arithmetic, linear algebra, matrix manipulation, and sampling. Each of these operations can be run on GUP, but by default it&amp;rsquo;s in CPU, we can move by  &lt;code&gt;.to&lt;/code&gt;  like the following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cuda&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;is_available&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;cuda&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here are some common operations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Indexing and Slicing&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# get the first row of tensor&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# get the first column of tensor&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# get the last column of tensor&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# change secound column to zero&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Joining tensors&lt;/p&gt;
&lt;p&gt;we can concatenate a sequence of tensors by a given dimension &lt;code&gt;torch.cat([t_data,t_data],dim=1)&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Arithmetic operations&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# computes the matrix multiplication&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matmul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matmul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;out&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# computes the element-wise product&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mul&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;out&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Single-element tensors&lt;/p&gt;
&lt;p&gt;If you have one-element tensor, can convert it to python value by &lt;code&gt;t_data.item()&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In place operations&lt;/p&gt;
&lt;p&gt;Sometime we would like to change tensors them self, instead of return an answer. In order to do this we can add  &lt;code&gt;_&lt;/code&gt; behind the function.&lt;/p&gt;
&lt;p&gt;For example:&lt;code&gt;t_data.add_(5)&lt;/code&gt;will add five for all element it self.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>📚Using Custom Dataset in PyTorch</title>
      <link>https://chengaoshen.com/blogs/using-custom-dataset-in-pytorch/</link>
      <pubDate>Wed, 26 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/blogs/using-custom-dataset-in-pytorch/</guid>
      <description>&lt;p&gt;In order to decouple dataset code and model code, PyTorch provides two data primitives: &lt;code&gt;torch.utils.data.DataLoader&lt;/code&gt; and &lt;code&gt;torch.utils.data.Dataset&lt;/code&gt;. &lt;code&gt;Dataset&lt;/code&gt; stores the samples and their corresponding labels, and &lt;code&gt;DataLoader&lt;/code&gt; wraps an iterable around the &lt;code&gt;Dataset&lt;/code&gt; to enable easy access to the samples.&lt;/p&gt;
&lt;h2 id=&#34;load-a-dataset&#34;&gt;Load a Dataset&lt;/h2&gt;
&lt;p&gt;If we want to use Data, we must have Data first. Fortunately, PyTorch domain libraries provide a number of pre-loaded datasets. All of them is the subclass of the&lt;code&gt;Dataset&lt;/code&gt;. Now we use Fashion-MNIST, one of them, to show how to load a dataset.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch.utils.data&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Dataset&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torchvision&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datasets&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torchvision.transforms&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt;  &lt;span class=&#34;nn&#34;&gt;ToTensor&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;training_data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datasets&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;FashionMNIST&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;n&#34;&gt;root&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;data&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# the path where the data is stored&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;train&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# specifies training or test dataset&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;download&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# downloads the data from internet if not available at root&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;transform&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ToTensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# specify the feature and label transformation&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;22-iterating-and-visualizing-the-dataset&#34;&gt;2.2 Iterating and Visualizing the Dataset&lt;/h3&gt;
&lt;p&gt;We can index &lt;code&gt;Datasets&lt;/code&gt; manually like a list:&lt;code&gt;training_data[index]&lt;/code&gt;, and use &lt;code&gt;matplotlib&lt;/code&gt; to visualizing photo data. If we want to display some element of Fashion-MNIST, just like followed:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;figure&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;figure&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;figsize&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;cols&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rows&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cols&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rows&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;sample_idx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randint&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;training_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,))&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;item&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;img&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;label&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;training_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sample_idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;figure&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;add_subplot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rows&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cols&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;off&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imshow&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;img&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;squeeze&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cmap&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;gray&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;show&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;23-creating-a-custom-dataset&#34;&gt;2.3 Creating a Custom Dataset&lt;/h3&gt;
&lt;p&gt;Sometimes,  prepared datasets can&amp;rsquo;t satisfy our need, so we can create a custom dataset which is subclass of &lt;code&gt;Dataset&lt;/code&gt;. A custom Dataset class must implement three functions:&lt;code&gt;__init__&lt;/code&gt;,&lt;code&gt;__len__&lt;/code&gt;,&lt;code&gt;__getitem__&lt;/code&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;__init__&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This function will run when instantiating the Dataset object. We can initialize a directory containing file and both transforms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;__len__&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This function should return the number of samples.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;__getitem__&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This function will returns a sample from the dataset at the given index. Some time we should return the element transformed.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So if we need to create a custom dataset, we can do as followed:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torchvision.io&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;read_image&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;CustomImageDataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;annotations_file&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;img_dir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;transform&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;target_transform&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;img_labels&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;annotations_file&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;img_dir&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;img_dir&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transform&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;transform&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;target_transform&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;target_transform&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__len__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;img_labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__getitem__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;img_path&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;img_dir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;img_labels&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iloc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;image&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;read_image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;img_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;label&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;img_labels&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iloc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;image&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;target_transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;label&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;target_transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;24-preparing-data-for-training&#34;&gt;2.4 Preparing data for training&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;Dataset&lt;/code&gt; retrieves our dataset&amp;rsquo;s feature and labels, but while we training a model, we need the pass samples in &amp;ldquo;minibatches&amp;rdquo;, reshuffle the data in every phase to reduce model overfitting. Sometimes we also need to use Python&amp;rsquo;s &lt;code&gt;multiprocessing&lt;/code&gt; to speed up our train. In order to do all of below, we can use &lt;code&gt;DataLoader&lt;/code&gt; class.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch.utils.data&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DataLoader&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;train_dataloader&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DataLoader&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;training_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shuffle&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;test_dataloader&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DataLoader&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;test_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;shuffle&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;After loaded that dataset into the DataLoader, we can iterate through all data.  Each iteration below return a batch(containing &lt;code&gt;batch_size&lt;/code&gt; feature) of &lt;code&gt;train_features&lt;/code&gt; and &lt;code&gt;train_labels&lt;/code&gt;. If &lt;code&gt;shuffle=True&lt;/code&gt; the data will shuffle after we iterate over all batches.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# get image and label.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;train_features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;train_labels&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;next&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;iter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train_dataloader&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Feature batch shape: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train_features&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Labels batch shape: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train_labels&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;img&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;train_features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;squeeze&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;label&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;train_labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;25-transforms&#34;&gt;2.5 Transforms&lt;/h3&gt;
&lt;p&gt;Sometimes data is not at the struct required for training machine learning algorithms, so we should &lt;strong&gt;transforms&lt;/strong&gt; the data to more suitable struct.&lt;/p&gt;
&lt;p&gt;For example the datasets in TorchVision always have two parameters(&lt;code&gt;transform&lt;/code&gt;:modify the features, &lt;code&gt;target_transform&lt;/code&gt;:modify the labels) that accept the transformation logic(offered in&lt;code&gt;torchvision.transforms&lt;/code&gt; module).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# we use FashionMNIST which feature are PIL Image format and the labels are integers&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# we needed to change feature as normalized tensors and labels as one-hot encoded tensors.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torchvision&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datasets&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torchvision.transforms&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ToTensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Lambda&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;ds&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datasets&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;FashionMNIST&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;root&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;data&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;download&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;transform&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ToTensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;target_transform&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Lambda&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;lambda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;float&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scatter_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;ToTensor&lt;/code&gt;:converts a PIL image or NumPy array into a Tensor,scales the image&amp;rsquo;s pixel intensity values in the range [0., 1.]&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Lambda Transforms&lt;/code&gt;:Lambda transforms apply any user-defined lambda function.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Blogs</title>
      <link>https://chengaoshen.com/blog/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/blog/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
