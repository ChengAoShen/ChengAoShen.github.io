<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ChengAo Shen</title>
    <link>https://chengaoshen.com/</link>
    <description>Recent content on ChengAo Shen</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sun, 12 Oct 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://chengaoshen.com/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>SVTime: Small Time Series Forecasting Models Informed by &#34;Physics&#34; of Large Vision Model Forecasters</title>
      <link>https://chengaoshen.com/en/publications/svtime/</link>
      <pubDate>Sun, 12 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/publications/svtime/</guid>
      <description>&lt;p&gt;Abstract: Time series AI is crucial for analyzing dynamic web content, driving a surge of pre-trained large models known for their strong knowledge encoding and transfer capabilities across diverse tasks. However, given their energy-intensive training, inference, and hardware demands, using large models as a one-fits-all solution raises serious concerns about carbon footprint and sustainability. For a specific task, a compact yet specialized, high-performing model may be more practical and affordable, especially for resource-constrained users such as small businesses. This motivates the question: Can we build cost-effective lightweight models with large-model-like performance on core tasks such as forecasting? This paper addresses this question by introducing SVTime, a novel Small model inspired by large Vision model (LVM) forecasters for long-term Time series forecasting (LTSF). Recently, LVMs have been shown as powerful tools for LTSF. We identify a set of key inductive biases of LVM forecasters &amp;ndash; analogous to the &amp;ldquo;physics&amp;rdquo; governing their behaviors in LTSF &amp;ndash; and design small models that encode these biases through meticulously crafted linear layers and constraint functions. Across 21 baselines spanning lightweight, complex, and pre-trained large models on 8 benchmark datasets, SVTime outperforms state-of-the-art (SOTA) lightweight models and rivals large models with 10^3 fewer parameters than LVMs, while enabling efficient training and inference in low-resource settings.&lt;/p&gt;</description>
    </item>
    <item>
      <title>‚úèÔ∏èGumbel Softmax</title>
      <link>https://chengaoshen.com/en/posts/gumbel-softmax/</link>
      <pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/posts/gumbel-softmax/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Motivation.&lt;/strong&gt; In many models we need to &lt;em&gt;select&lt;/em&gt; a discrete option inside the computation graph (e.g., pick one branch of a network). A hard argmax is non-differentiable, so gradients can‚Äôt flow through it. Gumbel-Softmax provides a continuous, differentiable approximation to this discrete sampling step.&lt;/p&gt;&#xA;&lt;h2 id=&#34;gumbel-max-trick&#34;&gt;&#xA;  &lt;strong&gt;Gumbel-Max Trick&lt;/strong&gt;&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#gumbel-max-trick&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Assume we have discrete distribution&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;$X$&lt;/th&gt;&#xA;          &lt;th&gt;1&lt;/th&gt;&#xA;          &lt;th&gt;2&lt;/th&gt;&#xA;          &lt;th&gt;3&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;$p$&lt;/td&gt;&#xA;          &lt;td&gt;0.2&lt;/td&gt;&#xA;          &lt;td&gt;0.3&lt;/td&gt;&#xA;          &lt;td&gt;0.5&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;And want to get $X$ follow this distribution. If we directly sample from distribution, the $X$ can‚Äôt calculate from $p$. Which means $X$ can‚Äôt be differentiated w.r.t. $p$. This means we can‚Äôt do back propagation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>üìÉDifferent Normalization</title>
      <link>https://chengaoshen.com/en/posts/different-normalization/</link>
      <pubDate>Mon, 21 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/posts/different-normalization/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;&#xA;  Introduction&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#introduction&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Normalization techniques are fundamental to training deep learning models effectively. They help &lt;strong&gt;stabilize and accelerate training&lt;/strong&gt;, &lt;strong&gt;improve generalization&lt;/strong&gt;, and &lt;strong&gt;prevent internal covariate shift&lt;/strong&gt;. Below is a summary of the &lt;strong&gt;most common normalization techniques&lt;/strong&gt;, their &lt;strong&gt;mechanisms&lt;/strong&gt;, &lt;strong&gt;key papers&lt;/strong&gt;, and &lt;strong&gt;differences&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/Normalization.png&#34; alt=&#34;image_normalization&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;-summary-of-different-type-of-normalization&#34;&gt;&#xA;  &lt;strong&gt;üîë Summary of different type of Normalization&lt;/strong&gt;&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#-summary-of-different-type-of-normalization&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Name&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Normalized Over&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Key Paper&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Common Use Cases&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;Strength&lt;/th&gt;&#xA;          &lt;th&gt;Weakness&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Batch Normalization (BN)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;For Conv: per channel across &lt;strong&gt;B√óH√óW&lt;/strong&gt;; For MLP: per feature across &lt;strong&gt;B.&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1502.03167&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/em&gt;&lt;/a&gt; (ICML 2015)&lt;/td&gt;&#xA;          &lt;td&gt;Computer Vision Field like Image Classification, Detection, Segmentation&lt;/td&gt;&#xA;          &lt;td&gt;Stabilizes activation scale; Enables larger learning rates, Speeds convergence;  Adds implicit regularization&lt;/td&gt;&#xA;          &lt;td&gt;Less suited to online / streaming / RNN small-batch settings, can cause issues in domain shift or micro-batch training&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Layer Normalization (LN)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Per &lt;em&gt;sample&lt;/em&gt; (token) across its &lt;strong&gt;feature (hidden) dimensions&lt;/strong&gt; (e.g. For shape B√óL√óD or B√óD: normalize over D; for Conv rarely used, would be over C√óH√óW of that sample)&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1607.06450&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Layer Normalization&lt;/em&gt;&lt;/a&gt; (arXiv 2016)&lt;/td&gt;&#xA;          &lt;td&gt;Transformers (NLP &amp;amp; Vision), RNNs, small-batch or batch=1 training&lt;/td&gt;&#xA;          &lt;td&gt;Independent of batch size, identical behavior in training &amp;amp; inference, stable for variable-length sequences, improves gradient flow (esp. Pre-LN Transformers)&lt;/td&gt;&#xA;          &lt;td&gt;Provides less implicit regularization, does not leverage cross-sample statistics&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Instance Normalization (IN)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;For conv input BxCxHxW: &lt;strong&gt;each sample &amp;amp; channel independently over its spatial pixels&lt;/strong&gt; HxW (no cross-batch, no cross-channel).&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1607.08022&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Instance Normalization: The Missing Ingredient for Fast Stylization&lt;/em&gt;&lt;/a&gt; (ECCV 2016)&lt;/td&gt;&#xA;          &lt;td&gt;image generation (GAN generators), image-to-image translation (e.g., style/appearance adaptation)&lt;/td&gt;&#xA;          &lt;td&gt;Batch size‚Äìindependent, effectively strips instance-specific style (contrast, color cast), aiding fast stylization&lt;/td&gt;&#xA;          &lt;td&gt;Discards global intensity/contrast cues useful for recognition ‚Üí poorer performance on classification/detection; lacks batch-level regularization&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Group Normalization (GN)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;For input BxCxHxW: &lt;strong&gt;per sample&lt;/strong&gt;, split channels into G groups (size C/G); compute mean &amp;amp; var over (C/G)xHxW inside each group.&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1803.08494&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Group Normalization&lt;/em&gt;&lt;/a&gt; (ECCV 2018)&lt;/td&gt;&#xA;          &lt;td&gt;Small-/micro-batch CNN training, cases where BN fails with batch sizes 1‚Äì4.&lt;/td&gt;&#xA;          &lt;td&gt;Batch-size independent; stable for tiny or variable batches; often better than BN when batch is very small.&lt;/td&gt;&#xA;          &lt;td&gt;Extra hyperparameter (G) to tune; less implicit regularization than BN, grouping may not align with the semantic channel structure&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Weight Normalization (WN)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Each weight vector of a neuron/output channel.&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1602.07868&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks&lt;/em&gt;&lt;/a&gt; (NIPS 2016)&lt;/td&gt;&#xA;          &lt;td&gt;RNN / seq models where BN is hard, small-batch or online / RL training (policy &amp;amp; value nets)&lt;/td&gt;&#xA;          &lt;td&gt;Negligible inference cost (can fold into static weights); works with streaming / RL; complements other norms (can combine with LayerNorm)&lt;/td&gt;&#xA;          &lt;td&gt;Scale may drift (need LR tuning); benefit can vanish with strong adaptive optimizers; less helpful for very deep Transformers (other norms preferred)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Spectral Normalization (SN)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Each weight tensor (e.g. matrix / conv kernel reshaped to 2D)&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1802.05957&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Spectral Normalization for Generative Adversarial Networks&lt;/em&gt;&lt;/a&gt; (ICLR 2018)&lt;/td&gt;&#xA;          &lt;td&gt;GAN discriminators, robustness / Lipschitz-constrained models, etc.&lt;/td&gt;&#xA;          &lt;td&gt;Enforces (approx.) 1-Lipschitz per layer (controls gradient explosion)&lt;/td&gt;&#xA;          &lt;td&gt;Extra cost (power iteration each step); only constrains the largest singular value (other singular values can still drift)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;RMS Normalization&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Per sample (token) feature vector&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.07467&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Root Mean Square Layer Normalization&lt;/em&gt;&lt;/a&gt; (NIPS 2019)&lt;/td&gt;&#xA;          &lt;td&gt;Modern Transformer / LLM blocks;  very deep pre-norm architectures, low-precision (FP16/BF16)&lt;/td&gt;&#xA;          &lt;td&gt;Simpler &amp;amp; slightly cheaper than LayerNorm, numerically stable in mixed precision, good for very deep stacks (retains strong gradient path)&lt;/td&gt;&#xA;          &lt;td&gt;Mean not zeroer,  possible drift, needs careful init/residual scaling, and isn‚Äôt fully interchangeable with zero-mean LN methods.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;-explanation-of-how-they-work&#34;&gt;&#xA;  &lt;strong&gt;üìò Explanation of How They Work&lt;/strong&gt;&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#-explanation-of-how-they-work&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;batch-normalization-bn&#34;&gt;&#xA;  &lt;strong&gt;Batch Normalization (BN)&lt;/strong&gt;&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#batch-normalization-bn&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;The Batch Normalization normally used in computer vision field, typically the CNN. Generally, the input shape of BN is  $\text{Batch}(B)\times \text{Channel}(C) \times \text{Height}(H)\times\text{Width}(W)$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?</title>
      <link>https://chengaoshen.com/en/publications/av4ts/</link>
      <pubDate>Mon, 02 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/publications/av4ts/</guid>
      <description>&lt;p&gt;Abstract: Transformer-based models have gained increasing attention in time series research, driving interest in Large Language Models (LLMs) and foundation models for time series analysis. As the field moves toward multi-modality, Large Vision Models (LVMs) are emerging as a promising direction. In the past, the effectiveness of Transformer and LLMs in time series has been debated. When it comes to LVMs, a similar question arises: are LVMs truely useful for time series analysis? To address it, we design and conduct the first principled study involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across both high-level (classification) and low-level (forecasting) tasks, with extensive ablation analysis. Our findings indicate LVMs are indeed useful for time series classification but face challenges in forecasting. Although effective, the contemporary best LVM forecasters are limited to specific types of LVMs and imaging methods, exhibit a bias toward forecasting periods, and have limited ability to utilize long look-back windows. We hope our findings could serve as a cornerstone for future research on LVM- and multimodal-based solutions to different time series tasks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multi-Modal View Enhanced Large Vision Models for Long-Term Time Series Forecasting</title>
      <link>https://chengaoshen.com/en/publications/dmmv/</link>
      <pubDate>Sun, 01 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/publications/dmmv/</guid>
      <description>&lt;p&gt;Abstract: Time series, typically represented as numerical sequences, can also be transformed into images and texts, offering multi-modal views (MMVs) of the same underlying signal. These MMVs can reveal complementary patterns and enable the use of powerful pre-trained large models, such as large vision models (LVMs), for long-term time series forecasting (LTSF). However, as we identified in this work, applying LVMs to LTSF poses an inductive bias towards &amp;ldquo;forecasting periods&amp;rdquo;. To harness this bias, we propose DMMV, a novel decomposition-based multi-modal view framework that leverages trend-seasonal decomposition and a novel backcast residual based adaptive decomposition to integrate MMVs for LTSF. Comparative evaluations against 14 state-of-the-art (SOTA) models across diverse datasets show that DMMV outperforms single-view and existing multi-modal baselines, achieving the best mean squared error (MSE) on 6 out of 8 benchmark datasets.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploring Multi-Modal Data with Tool-Augmented LLM Agents for Precise Causal Discovery</title>
      <link>https://chengaoshen.com/en/publications/matmcd/</link>
      <pubDate>Thu, 15 May 2025 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/publications/matmcd/</guid>
      <description>&lt;p&gt;Abstract: Causal discovery is an imperative foundation for decision-making across domains, such as smart health, AI for drug discovery and AIOps. Traditional statistical causal discovery methods, while well-established, predominantly rely on observational data and often overlook the semantic cues inherent in cause-and-effect relationships. The advent of Large Language Models (LLMs) has ushered in an affordable way of leveraging the semantic cues for knowledge-driven causal discovery, but the development of LLMs for causal discovery lags behind other areas, particularly in the exploration of multi-modal data. To bridge the gap, we introduce MATMCD, a multi-agent system powered by tool-augmented LLMs. MATMCD has two key agents: a Data Augmentation agent that retrieves and processes modality-augmented data, and a Causal Constraint agent that integrates multi-modal data for knowledge-driven reasoning. The proposed design of the inner-workings ensures successful cooperation of the agents. Our empirical study across seven datasets suggests the significant potential of multi-modality enhanced causal discovery.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Harnessing Vision Models for Time Series Analysis: A Survey</title>
      <link>https://chengaoshen.com/en/publications/v4ts_survey/</link>
      <pubDate>Tue, 15 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/publications/v4ts_survey/</guid>
      <description>&lt;p&gt;Abstract: Time series analysis has witnessed the inspiring development from traditional autoregressive models, deep learning models, to recent Transformers and Large Language Models (LLMs). Efforts in leveraging vision models for time series analysis have also been made along the way but are less visible to the community due to the predominant research on sequence modeling in this domain. However, the discrepancy between continuous time series and the discrete token space of LLMs, and the challenges in explicitly modeling the correlations of variates in multivariate time series have shifted some research attentions to the equally successful Large Vision Models (LVMs) and Vision Language Models (VLMs). To fill the blank in the existing literature, this survey discusses the advantages of vision models over LLMs in time series analysis. It provides a comprehensive and in-depth overview of the existing methods, with dual views of detailed taxonomy that answer the key research questions including how to encode time series as images and how to model the imaged time series for various tasks. Additionally, we address the challenges in the pre- and post-processing steps involved in this framework and outline future directions to further advance time series analysis with vision models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>üìúNotes on LaTeX formulas</title>
      <link>https://chengaoshen.com/en/posts/various-font-in-latex/</link>
      <pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/posts/various-font-in-latex/</guid>
      <description>&lt;p&gt;$\LaTeX$ is a document preparation system used for the communication and publication of scientific documents. However, it&amp;rsquo;s not easy to use this to write formulas, especially for the beginners, typically how to choose the right font and Greek letter.&lt;/p&gt;&#xA;&lt;p&gt;This essay is a note for me to remember the latex formulas, which have vairous fonts and Greek letter. Furthermore I also record some special symbols that can be used in the Future papers.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ü§óIntroduction to Generative Models</title>
      <link>https://chengaoshen.com/en/posts/introduction-to-generative-models/</link>
      <pubDate>Mon, 26 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/posts/introduction-to-generative-models/</guid>
      <description>&lt;p&gt;Generative Models are part of unsupervised learning models that can learned from the datasets without any labels. Unlike other unsupervised models to manipulate, denoise, interpolate between, or compress examples, generative models focus on generating plausible new samples having similar properties to the dataset.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20231025211322464.png&#34; alt=&#34;Taxonomy of unsupervised learning models&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Latent variable models&lt;/strong&gt;: mapping the data examples $\mathbf{x}$ to unseen latent variables $\mathbf{z}$ which can capture the underlying structure in the dataset.&lt;/p&gt;</description>
    </item>
    <item>
      <title>‚öΩÔ∏èIntroduction to Prompt Engineering</title>
      <link>https://chengaoshen.com/en/posts/introduction-to-prompt-engineering/</link>
      <pubDate>Mon, 15 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/posts/introduction-to-prompt-engineering/</guid>
      <description>&lt;h2 id=&#34;basic-knowledge&#34;&gt;&#xA;  Basic Knowledge&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#basic-knowledge&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Prompt engineering&lt;/strong&gt; is a relatively new discipline for developing and optimizing prompts to efficiently use large lange models (LLMs) for a wide variety of applications and research topics. Researchers use prompt engineering to improve the safety and the capacity of LLMs on a wide range of common and complex tasks such as question answering and arithmetic reasoning. Developers use prompt engineering to design robust and effective prompting techniques that interface with LLMs and other tools.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Emoji Kitchen with Controlled Fusion</title>
      <link>https://chengaoshen.com/en/publications/emoji-kitchen/</link>
      <pubDate>Tue, 19 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/publications/emoji-kitchen/</guid>
      <description>&lt;p&gt;Abstract: The image fusion method is widely used in many different fields. The fusion processes both need models to extract semantic information and contain details. Traditional image processing techniques used for this issue have limited ability to extract semantic features from images, and advanced deep learning techniques often lose the details. In this work, we propose the Controlled Fusion Network (CFN) that adopts a multi-step progressive generation method and injects control elements at every step. We test the model in the emoji fusion task which accepts various emojis and combines them. We find that the generated emojis sufficiently retain and reasonably combine the semantic information of the input images, while the result images also conform to human intuitive perception. Our source code is released at: &lt;a href=&#34;https://github.com/ChengAoShen/Emoji_fusion&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/ChengAoShen/Emoji_fusion&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>üìöUsing Custom Dataset in PyTorch</title>
      <link>https://chengaoshen.com/en/posts/using-custom-dataset-in-pytorch/</link>
      <pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/posts/using-custom-dataset-in-pytorch/</guid>
      <description>&lt;p&gt;In order to decouple dataset code and model code, PyTorch provides two data primitives: &lt;code&gt;torch.utils.data.DataLoader&lt;/code&gt; and &lt;code&gt;torch.utils.data.Dataset&lt;/code&gt;. &lt;code&gt;Dataset&lt;/code&gt; stores the samples and their corresponding labels, and &lt;code&gt;DataLoader&lt;/code&gt; wraps an iterable around the &lt;code&gt;Dataset&lt;/code&gt; to enable easy access to the samples.&lt;/p&gt;&#xA;&lt;h2 id=&#34;load-a-dataset&#34;&gt;&#xA;  Load a Dataset&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#load-a-dataset&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;If we want to use Data, we must have Data first. Fortunately, PyTorch domain libraries provide a number of pre-loaded datasets. All of them is the subclass of the&lt;code&gt;Dataset&lt;/code&gt;. Now we use Fashion-MNIST, one of them, to show how to load a dataset.&lt;/p&gt;</description>
    </item>
    <item>
      <title>üí°Introduction to Transformer</title>
      <link>https://chengaoshen.com/en/posts/introduction-to-transformer/</link>
      <pubDate>Mon, 23 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/posts/introduction-to-transformer/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;Transformer is a really popular method in modern neural networks. We have BERT or GPT to process the natural language and ViT to deal with computer vision. In this essay, you will understand what is the transformer and why the transformer works. But be careful, limited by my knowledge, I can‚Äôt show some mathematical theories or code of transformer for you.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;h2 id=&#34;why-do-we-need-the-transformer&#34;&gt;&#xA;  Why do we need the Transformer?&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#why-do-we-need-the-transformer&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;In the NLP( Natural Language Processing) field, the text dataset always has some obvious features that prevent us from using MLP.&lt;/p&gt;</description>
    </item>
    <item>
      <title>üìùCommon PyTorch Code Snippets</title>
      <link>https://chengaoshen.com/en/posts/common-pytorch-code-snippets/</link>
      <pubDate>Tue, 21 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/posts/common-pytorch-code-snippets/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;Notes inspired by d2l&amp;rsquo;s PyTorch content&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;h2 id=&#34;device-switching&#34;&gt;&#xA;  Device Switching&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#device-switching&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Try using GPU:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;try_gpu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cuda&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device_count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;cuda:&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;cpu&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;try_all_gpus&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Find all available GPUs&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;devices&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;cuda:&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;               &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cuda&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device_count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;devices&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;devices&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;cpu&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;accumulator&#34;&gt;&#xA;  Accumulator&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#accumulator&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;Accumulator&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Used for accumulating values&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;add&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Add a list of values to the existing data&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;float&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;reset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__getitem__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;accuracy&#34;&gt;&#xA;  Accuracy&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#accuracy&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;For training accuracy evaluation, you can use the following code&lt;/p&gt;</description>
    </item>
    <item>
      <title>üñäÔ∏èEffective Python Tips</title>
      <link>https://chengaoshen.com/en/posts/effective-python-tips/</link>
      <pubDate>Sat, 21 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/posts/effective-python-tips/</guid>
      <description>&lt;h2 id=&#34;1-pythonic-thinking&#34;&gt;&#xA;  1. Pythonic Thinking&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#1-pythonic-thinking&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;11-follow-pep8-style&#34;&gt;&#xA;  1.1 Follow PEP8 Style&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#11-follow-pep8-style&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Regarding Naming&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Functions, variables, and attributes use lowercase letters, with words separated by underscores.&lt;/li&gt;&#xA;&lt;li&gt;Protected instance attributes start with a single underscore.&lt;/li&gt;&#xA;&lt;li&gt;Private instance attributes should start with two underscores.&lt;/li&gt;&#xA;&lt;li&gt;Class (and exception) names should have the first letter capitalized.&lt;/li&gt;&#xA;&lt;li&gt;Module-level constants are all uppercase letters, with words separated by underscores.&lt;/li&gt;&#xA;&lt;li&gt;Instance methods in classes should name the first parameter &lt;code&gt;self&lt;/code&gt; to represent the object itself.&lt;/li&gt;&#xA;&lt;li&gt;The first parameter of class methods should be named &lt;code&gt;cls&lt;/code&gt; to represent the class itself.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Regarding Expressions&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Use inline negation (write the negation directly before the content to be negated). For example: &lt;code&gt;if a is not b&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;To check if something is empty, you can directly test it (empty is False). For example: &lt;code&gt;if not somelist&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;If an expression does not fit on one line, it should be enclosed in parentheses and broken into multiple lines.&lt;/li&gt;&#xA;&lt;li&gt;Use parentheses to continue multi-line expressions instead of the backslash &lt;code&gt;\&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Regarding Imports&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;code&gt;import&lt;/code&gt; statements should be placed at the top.&lt;/li&gt;&#xA;&lt;li&gt;Use absolute names for imports.&lt;/li&gt;&#xA;&lt;li&gt;When importing, first divide into three parts: standard library, third-party modules, and your own modules.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;12-use-f-strings-for-string-interpolation&#34;&gt;&#xA;  1.2 Use f-strings for String Interpolation&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#12-use-f-strings-for-string-interpolation&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;‚ÄÉ‚ÄÉWhen formatting strings, you can use Python&amp;rsquo;s special &lt;strong&gt;f-string&lt;/strong&gt; method.&lt;/p&gt;</description>
    </item>
    <item>
      <title>‚öïÔ∏èBrief introduction of the Tensor in PyTorch</title>
      <link>https://chengaoshen.com/en/posts/brief-introduction-of-the-tensor-in-pytorch/</link>
      <pubDate>Wed, 26 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/posts/brief-introduction-of-the-tensor-in-pytorch/</guid>
      <description>&lt;p&gt;Tensor is a specialized data structure that is very similar to arrays and matrices. We can use it to encode the input and output of the model. Tensors can run on GPUs and other hardware.&lt;/p&gt;&#xA;&lt;h2 id=&#34;initializing-a-tensors&#34;&gt;&#xA;  Initializing a Tensors&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#initializing-a-tensors&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Tensors can be initialized in various ways,&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Import the library&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Directly from data&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# From Numpy&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;np_data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;t_np&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from_numpy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;np_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# From other tensors&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# In this way, it will retains same properties&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;t_ones&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ones_like&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# override the datatype&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;t_random&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rand_like&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;float&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# With static shape&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;rand_t&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rand&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;ones_t&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ones&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;zeros_t&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;Careful&lt;/strong&gt;: If you initialize the tensors from Numpy, they will share the same underlying memory, which means that if changing the numpy array, the tensor will change too.&lt;/p&gt;</description>
    </item>
    <item>
      <title>‚ú®Organizing C Pointer Declarations</title>
      <link>https://chengaoshen.com/en/posts/organizing-c-pointer-declarations/</link>
      <pubDate>Mon, 30 May 2022 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/posts/organizing-c-pointer-declarations/</guid>
      <description>&lt;h2 id=&#34;1-common-declarations&#34;&gt;&#xA;  1. Common Declarations&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#1-common-declarations&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;code&gt;int a&lt;/code&gt;: Declares &lt;code&gt;a&lt;/code&gt; as an integer variable.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;code&gt;int const a&lt;/code&gt;: Declares a constant integer &lt;code&gt;a&lt;/code&gt; (cannot be modified).&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;code&gt;int *a&lt;/code&gt;: Can be understood as &amp;ldquo;&lt;code&gt;*a&lt;/code&gt; is an integer&amp;rdquo;, so &lt;code&gt;a&lt;/code&gt; is a pointer to an integer.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;code&gt;int **a&lt;/code&gt;: Since &amp;ldquo;&lt;code&gt;**a&lt;/code&gt; is an integer&amp;rdquo;, &lt;code&gt;a&lt;/code&gt; is a pointer to a pointer to an integer.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Note: &lt;code&gt;int *a, b;&lt;/code&gt; declares &lt;code&gt;a&lt;/code&gt; as a pointer to an integer, but &lt;code&gt;b&lt;/code&gt; is just an integer‚Äînot a pointer.&lt;/p&gt;</description>
    </item>
    <item>
      <title>‚òÅÔ∏èGit Notes(Chinese)</title>
      <link>https://chengaoshen.com/en/posts/git-noteschinese/</link>
      <pubDate>Mon, 30 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/posts/git-noteschinese/</guid>
      <description>&lt;h2 id=&#34;‰∏ÄÂáÜÂ§áÂ∑•‰Ωú&#34;&gt;&#xA;  ‰∏Ä„ÄÅÂáÜÂ§áÂ∑•‰Ωú&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#%e4%b8%80%e5%87%86%e5%a4%87%e5%b7%a5%e4%bd%9c&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;11-ÂÆâË£Ögit&#34;&gt;&#xA;  1.1 ÂÆâË£ÖGit&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#11-%e5%ae%89%e8%a3%85git&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;‚Äã&#x9;Âú®&lt;code&gt;https://git-scm.com/&lt;/code&gt;‰∏ä‰∏ãËΩΩÂÆâË£ÖÂåÖÂêéÂÇªÁìúÂºèÂÆâË£Ö„ÄÇ&lt;/p&gt;&#xA;&lt;h3 id=&#34;12-ÂàùÂßãÂåñgit&#34;&gt;&#xA;  1.2 ÂàùÂßãÂåñGit&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#12-%e5%88%9d%e5%a7%8b%e5%8c%96git&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;‚Äã&#x9;ÂàùÂßãÂåñÁî®Êà∑&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git config --global user.name &lt;span class=&#34;s2&#34;&gt;&amp;#34;your name&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git config --global user.email &lt;span class=&#34;s2&#34;&gt;&amp;#34;your email&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;‚Äã&#x9;ÊèêÈ´òËæìÂá∫ÂèØËØªÊÄß&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git config --global color.ui.auto&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;13-ÂáÜÂ§ágithub&#34;&gt;&#xA;  1.3 ÂáÜÂ§áGitHub&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#13-%e5%87%86%e5%a4%87github&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Ê≥®ÂÜåË¥¶Êà∑„ÄÇ&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;ËÆæÁΩÆSSH Key&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ ssh-keygen -t rsa -C &lt;span class=&#34;s2&#34;&gt;&amp;#34;your email&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Generating public/private rsa key pair.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Enter file in which to save the key &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;C:&lt;span class=&#34;se&#34;&gt;\U&lt;/span&gt;sers&lt;span class=&#34;se&#34;&gt;\2&lt;/span&gt;6411/.ssh/id_rsa&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;:ÔºàÁõ¥Êé•ÂõûËΩ¶Ôºâ&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Created directory &lt;span class=&#34;s1&#34;&gt;&amp;#39;C:\Users\26411/.ssh&amp;#39;&lt;/span&gt;.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Enter passphrase &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;empty &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; no passphrase&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;:ÔºàËæìÂÖ•ÂØÜÁ†ÅÔºâ&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Enter same passphrase again:ÔºàÁ°ÆËÆ§ÂØÜÁ†ÅÔºâ&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;ÊàëÁöÑËÆ∞ÂΩï:&lt;/p&gt;</description>
    </item>
    <item>
      <title>‚ö°Ô∏èFastAPI Notes(Chinese)</title>
      <link>https://chengaoshen.com/en/posts/fastapi-noteschinese/</link>
      <pubDate>Sun, 21 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/posts/fastapi-noteschinese/</guid>
      <description>&lt;h2 id=&#34;ÂºïÂÖ•&#34;&gt;&#xA;  ÂºïÂÖ•&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#%e5%bc%95%e5%85%a5&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;‰ªãÁªç&#34;&gt;&#xA;  ‰ªãÁªç&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#%e4%bb%8b%e7%bb%8d&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;FastAPI ÊòØ‰∏Ä‰∏™Áî®‰∫éÊûÑÂª∫ API ÁöÑÁé∞‰ª£„ÄÅÂø´ÈÄüÔºàÈ´òÊÄßËÉΩÔºâÁöÑ web Ê°ÜÊû∂Ôºå‰ΩøÁî® Python 3.6+ Âπ∂Âü∫‰∫éÊ†áÂáÜÁöÑ Python Á±ªÂûãÊèêÁ§∫„ÄÇ&lt;/p&gt;&#xA;&lt;h3 id=&#34;ÂÆâË£Ö&#34;&gt;&#xA;  ÂÆâË£Ö&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#%e5%ae%89%e8%a3%85&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;ÂÆåÊï¥ÂÆâË£ÖÔºö&lt;code&gt;pip install &amp;quot;fastapi[all]&amp;quot;&lt;/code&gt;ÔºåËøôÁßçÂÆâË£ÖÊñπÂºèÂåÖÊã¨‰∫Üunicorn&lt;/p&gt;&#xA;&lt;p&gt;ÈÉ®ÂàÜÂÆâË£ÖÔºö&lt;code&gt;pip install fastapi&lt;/code&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;ÊúçÂä°ÁöÑÂêØÂä®&#34;&gt;&#xA;  ÊúçÂä°ÁöÑÂêØÂä®&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#%e6%9c%8d%e5%8a%a1%e7%9a%84%e5%90%af%e5%8a%a8&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;‰∏Ä‰∏™ÊúÄÁÆÄÂçïÁöÑFastAPIÂ¶Ç‰∏ãÊâÄÁ§∫Ôºö&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;fastapi&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;FastAPI&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;app&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;FastAPI&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nd&#34;&gt;@app.get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;async&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;root&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;message&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Hello World&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;ÈúÄË¶ÅÂ∞ÜËøô‰∏™Êñá‰ª∂ÊâÄÊèèËø∞ÁöÑÊúçÂä°ËøõË°åÂêØÂä®ÂèØ‰ª•‰ΩøÁî®ÂëΩ‰ª§Ôºö&lt;code&gt;uvicorn main:app --reload&lt;/code&gt;ÔºåÊúÄÂêé‰∏Ä‰∏™&lt;code&gt;reload&lt;/code&gt;Ë°®Á§∫ÊòØÂú®Êõ¥Êñ∞‰ª£Á†ÅÂêéÈáçÊñ∞ÂêØÂä®&lt;/p&gt;&#xA;&lt;h3 id=&#34;Êé•Âè£ÊñáÊ°£&#34;&gt;&#xA;  Êé•Âè£ÊñáÊ°£&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#%e6%8e%a5%e5%8f%a3%e6%96%87%e6%a1%a3&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Ëøô‰∏™Ê°ÜÊû∂‰ºöÂü∫‰∫éopenapiËá™Âä®ÁîüÊàêÊñáÊ°£ÔºåÂéüÂßãÁöÑÊñá‰ª∂Âú®&lt;code&gt;/openai.json&lt;/code&gt;‰∏≠ÔºåÂú®&lt;code&gt;/docs&lt;/code&gt;Âíå&lt;code&gt;/redoc&lt;/code&gt;‰∏≠ÂèØ‰ª•ÂàÜÂà´ÁúãÂà∞‰∏§‰∏™‰∏çÂêåÊòæÁ§∫ÁïåÈù¢ÁöÑÊñáÊ°£&lt;/p&gt;</description>
    </item>
    <item>
      <title>üê≥Docker Basic(Chinese)</title>
      <link>https://chengaoshen.com/en/posts/docker-basicchinese/</link>
      <pubDate>Wed, 30 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/posts/docker-basicchinese/</guid>
      <description>&lt;h2 id=&#34;‰∏ÄdokerÂø´ÈÄü‰ΩøÁî®&#34;&gt;&#xA;  ‰∏Ä„ÄÅDokerÂø´ÈÄü‰ΩøÁî®&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#%e4%b8%80doker%e5%bf%ab%e9%80%9f%e4%bd%bf%e7%94%a8&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;11-ÊûÑÂª∫‰∏Ä‰∏™ÂÆπÂô®&#34;&gt;&#xA;  1.1 ÊûÑÂª∫‰∏Ä‰∏™ÂÆπÂô®&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#11-%e6%9e%84%e5%bb%ba%e4%b8%80%e4%b8%aa%e5%ae%b9%e5%99%a8&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;ÊãâÂèñÂπ∂‰ΩøÁî®dockerÁöÑubuntuÈïúÂÉèÊûÑÂª∫‰∏Ä‰∏™&lt;strong&gt;‰∫§‰∫íÂºè&lt;/strong&gt;ÁöÑÂÆπÂô®„ÄÇ&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo docker run --name containerName -i -t ubuntu /bin/bash&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;&#xA;&lt;p&gt;ÂèÇÊï∞Ëß£ÈáäÔºö&lt;/p&gt;&#xA;&lt;p&gt;``&amp;ndash;name`ÊòØÂØπ‰∫éÂÆπÂô®ÁöÑÂëΩÂêç&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;-i&lt;/code&gt;‰øùËØÅÂÆπÂô®ÁöÑSTDINÂºÄÂêØÔºå‰øùËØÅÊåÅÁª≠ÁöÑÊ†áÂáÜËæìÂÖ•Ôºõ&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;-t&lt;/code&gt;‰∏∫docker‰∏∫ÂÆπÂô®ÂàÜÈÖç‰∏Ä‰∏™ttyÁªàÁ´Ø&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;ubuntu&lt;/code&gt;Êèê‰æõÁöÑÊòØÈïúÂÉèÂêçÔºåÂÖà‰ªéÊú¨Êú∫Êü•Êâæ‰πãÂêéÔºåËã•Êó†ÂàôÂéªdockerHub‰∏äÊü•Êâæ&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;/bin/bash&lt;/code&gt;ÂëäÁü•ÂÆπÂô®ÈúÄË¶ÅËøêË°åÁöÑÂëΩ‰ª§&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;h3 id=&#34;12-ÂÖ≥Èó≠ÂÆπÂô®‰∏éÊü•Áúã&#34;&gt;&#xA;  1.2 ÂÖ≥Èó≠ÂÆπÂô®‰∏éÊü•Áúã&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#12-%e5%85%b3%e9%97%ad%e5%ae%b9%e5%99%a8%e4%b8%8e%e6%9f%a5%e7%9c%8b&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Âú®ÂÆπÂô®ÂÜÖÈîÆÂÖ•&lt;code&gt;exit&lt;/code&gt;ÂèØ‰ª•ÈÄÄÂá∫ÂÆπÂô®Ôºå‰πãÂêéÂú®ÂÆø‰∏ªÊú∫‰∏äÂèØ‰ª•ÈÄöËøá‰ΩøÁî®&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;docker ps -a&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Êù•Êü•ÁúãÊâÄÊã•ÊúâÁöÑÂÖ®ÈÉ®ÂÆπÂô®„ÄÇ&lt;/p&gt;&#xA;&lt;h3 id=&#34;13-ÂêØÂä®Âπ∂ÈôÑÁùÄÂà∞ÂÆπÂô®&#34;&gt;&#xA;  1.3 ÂêØÂä®Âπ∂ÈôÑÁùÄÂà∞ÂÆπÂô®&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#13-%e5%90%af%e5%8a%a8%e5%b9%b6%e9%99%84%e7%9d%80%e5%88%b0%e5%ae%b9%e5%99%a8&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;ÂΩìÁü•ÈÅìÂÆπÂô®ÁöÑÂêçÂ≠óÊàñËÄÖuuidÁöÑÊó∂ÂÄôÂèØ‰ª•‰ΩøÁî®&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo docker start containerName&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo docker start UUID&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Êù•ÈáçÊñ∞ËøêË°åÂÆπÂô®ÔºåÈáçÊñ∞ËøêË°åÁöÑÂÆπÂô®‰ºöÊ≤øÁî®&lt;code&gt;docker run&lt;/code&gt;Êó∂ÂÄôÁöÑÂëΩ‰ª§ÔºåÊ≠§Êó∂Êàë‰ª¨ÂèØ‰ª•‰ΩøÁî®&lt;/p&gt;</description>
    </item>
    <item>
      <title>‚úèÔ∏èMarkdown Notes(Chinese)</title>
      <link>https://chengaoshen.com/en/posts/markdown-noteschinese/</link>
      <pubDate>Mon, 21 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/posts/markdown-noteschinese/</guid>
      <description>&lt;h2 id=&#34;‰ΩøÁî®‰ªÄ‰πàÊù•Â≠¶‰π†&#34;&gt;&#xA;  „Äá„ÄÅ‰ΩøÁî®‰ªÄ‰πàÊù•Â≠¶‰π†&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#%e4%bd%bf%e7%94%a8%e4%bb%80%e4%b9%88%e6%9d%a5%e5%ad%a6%e4%b9%a0&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;01-typora&#34;&gt;&#xA;  0.1 Typora&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#01-typora&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;‚Äã ÊâÄËßÅÂç≥ÊâÄÂæóÁöÑ‰∏ÄÊ¨æÂÖçË¥πËÄåÂ•ΩÁî®ÁöÑÁºñËæëÂô®„ÄÇ&lt;/p&gt;&#xA;&lt;h3 id=&#34;02-vs-code&#34;&gt;&#xA;  0.2 VS Code&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#02-vs-code&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;‚Äã ÂÆáÂÆôÊúÄÂº∫Â§ßÁöÑÁºñËæëÂô®&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;‰∏ÄÂü∫Á°ÄËØ≠Ê≥ï&#34;&gt;&#xA;  ‰∏Ä„ÄÅÂü∫Á°ÄËØ≠Ê≥ï&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#%e4%b8%80%e5%9f%ba%e7%a1%80%e8%af%ad%e6%b3%95&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;11-Ê†áÈ¢òÁöÑ‰ΩøÁî®&#34;&gt;&#xA;  1.1 Ê†áÈ¢òÁöÑ‰ΩøÁî®&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#11-%e6%a0%87%e9%a2%98%e7%9a%84%e4%bd%bf%e7%94%a8&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# ÊñáÊ°£Ê†áÈ¢ò&#xA;&#xA;‰ΩúËÄÖ&#xA;&#xA;ÊëòË¶Å&#xA;&#xA;ÁõÆÂΩï&#xA;&#xA;## Ê†áÈ¢ò1&#xA;&#xA;### Ê†áÈ¢ò1.1&#xA;&#xA;## Ê†áÈ¢ò2&#xA;&#xA;###Ê†áÈ¢ò2.2&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;12-Á≤ó‰Ωì‰∏éÊñú‰Ωì&#34;&gt;&#xA;  1.2 Á≤ó‰Ωì‰∏éÊñú‰Ωì&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#12-%e7%b2%97%e4%bd%93%e4%b8%8e%e6%96%9c%e4%bd%93&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Á≤ó‰Ωì&lt;/strong&gt;  &lt;em&gt;Êñú‰Ωì&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>üêçPython Basic(Chinese)</title>
      <link>https://chengaoshen.com/en/posts/python-basicchinese/</link>
      <pubDate>Fri, 21 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/posts/python-basicchinese/</guid>
      <description>&lt;h2 id=&#34;‰∏ÄÂ≠¶ÂâçÂáÜÂ§á&#34;&gt;&#xA;  ‰∏Ä„ÄÅÂ≠¶ÂâçÂáÜÂ§á&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#%e4%b8%80%e5%ad%a6%e5%89%8d%e5%87%86%e5%a4%87&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;11-ÂÆâË£Ö&#34;&gt;&#xA;  1.1 ÂÆâË£Ö&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#11-%e5%ae%89%e8%a3%85&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;‚Äã&#x9;&#x9;Âª∫ËÆÆÁõ¥Êé•‰∏ãËΩΩanacondaÁöÑÂèëË°åÁâà„ÄÇÂêåÊó∂ËøòÂåÖÊã¨‰∫ÜÊâÄÊúâ‰ª•Âêé‰ºöÁªèÂ∏∏‰ΩøÁî®Âà∞ÁöÑÂåÖ„ÄÇ&lt;/p&gt;&#xA;&lt;h3 id=&#34;12-ÁºñÁ®ãÁéØÂ¢É&#34;&gt;&#xA;  1.2 ÁºñÁ®ãÁéØÂ¢É&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#12-%e7%bc%96%e7%a8%8b%e7%8e%af%e5%a2%83&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;‚Äã&#x9;&#x9;ÂèØ‰ª•‰∏ãËΩΩÂπ∂‰ΩøÁî®pycharm„ÄÅ‰πüÂèØ‰ª•‰ΩøÁî®jupyter notebook‰Ωú‰∏∫ÁéØÂ¢É„ÄÇ&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;‰∫åpythonÁöÑËØ≠Ë®ÄÂü∫Á°Ä&#34;&gt;&#xA;  ‰∫å„ÄÅPythonÁöÑËØ≠Ë®ÄÂü∫Á°Ä&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#%e4%ba%8cpython%e7%9a%84%e8%af%ad%e8%a8%80%e5%9f%ba%e7%a1%80&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;21-Áº©Ëøõ&#34;&gt;&#xA;  2.1 Áº©Ëøõ&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#21-%e7%bc%a9%e8%bf%9b&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;‚Äã&#x9;&#x9;python‰∏≠Áº©ËøõÊòØ‰∏ÄÂàáÁöÑÊ†πÊú¨Ôºå‰ΩøÁî®Áº©ËøõÊù•Ê†áËØÜ‰∫ÜËØ≠Ë®ÄÁöÑËåÉÂõ¥&lt;/p&gt;&#xA;&lt;h3 id=&#34;22Ê†áÈáèÁ±ªÂûã&#34;&gt;&#xA;  2.2Ê†áÈáèÁ±ªÂûã&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#22%e6%a0%87%e9%87%8f%e7%b1%bb%e5%9e%8b&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;2.2.1 &lt;strong&gt;Êï∞ÂÄºÁ±ªÂûã&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;‚Äã&#x9;&#x9;python‰∏≠Âè™Êúâ‰∏§‰∏™Âü∫Êú¨Êï∞ÂÄºÁ±ªÂûãÔºöint‰∏éfloat&lt;/p&gt;&#xA;&lt;p&gt;‚Äã&#x9;&#x9;intÂèØ‰ª•ÂÇ®Â≠ò‰ªªÊÑèÂ§ßÂ∞èÁöÑÊï¥Êï∞ÂÄºËÄåÊ≤°ÊúâËåÉÂõ¥ÈôêÂà∂ÔºåfloatÊòØÂèåÁ≤æÂ∫¶ÁöÑ64‰ΩçÊï∞ÂÄº„ÄÇ&lt;/p&gt;</description>
    </item>
    <item>
      <title>üìäPython with Data Science(Chinese)</title>
      <link>https://chengaoshen.com/en/posts/python-with-data-sciencechinese/</link>
      <pubDate>Fri, 21 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/en/posts/python-with-data-sciencechinese/</guid>
      <description>&lt;h2 id=&#34;‰∏ÄnumpyÂü∫Á°Ä&#34;&gt;&#xA;  ‰∏Ä„ÄÅNumPyÂü∫Á°Ä&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#%e4%b8%80numpy%e5%9f%ba%e7%a1%80&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;code&gt;import numpy as np&lt;/code&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;11-ndarryÂ§öÁª¥Êï∞ÁªÑÂØπË±°&#34;&gt;&#xA;  1.1 ndarry‚Äî‚ÄîÂ§öÁª¥Êï∞ÁªÑÂØπË±°&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#11-ndarry%e5%a4%9a%e7%bb%b4%e6%95%b0%e7%bb%84%e5%af%b9%e8%b1%a1&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;1.1.1 &lt;strong&gt;ÂàõÂª∫ndarray&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Â∞ÜËæìÂÖ•ÁöÑÂ∫èÂàóÁ±ªÂûãËΩ¨Êç¢‰∏∫ndarray,Â¶ÇÊûú‰∏çÊåáÂÆöÁ±ªÂûãÂàôËá™Âä®Âà§Êñ≠&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;arange&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# ËøîÂõû‰∏Ä‰∏™Á≠âÂ∑ÆÊï∞Âàó&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;eye&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;identity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#ÁîüÊàê‰∏éÁªôÂÆöÂΩ¢Áä∂‰∏ÄÊ†∑ÁöÑÂÖ®1Êï∞ÁªÑ&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;ones&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#ÁîüÊàê‰∏éÁªôÂÆöÂΩ¢Áä∂‰∏ÄÊ†∑ÁöÑÂÖ®1Êï∞ÁªÑ&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#ÁîüÊàê‰∏éÁªôÂÆöÂΩ¢Áä∂‰∏ÄÊ†∑ÁöÑÂÖ®0Êï∞ÁªÑ&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;empty&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#ÁîüÊàê‰∏éÁªôÂÆöÂΩ¢Áä∂‰∏ÄÊ†∑ÁöÑÁ©∫Êï∞ÁªÑ&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;full&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#Ê†πÊçÆÁªôÂÆöÂΩ¢Áä∂ÂíåÊï∞ÊçÆÁ±ªÂûãÁîüÊàêÊï∞ÂÄº‰∏ÄÊ†∑ÁöÑÊï∞ÁªÑ&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#‰ª•‰∏äÁöÑÂáΩÊï∞ÂèØ‰ª•Âä†_like ÂèòÊàêÊ†πÊçÆ‰∏Ä‰∏™Êï∞ÁªÑÁöÑÂ§ßÂ∞èÊù•ÁîüÊàê&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#‰ΩøÂæó‰∏Ä‰∏™Â∫èÂàóËøõË°åarrayÊ†∑ÂºèÂèòÂåñ&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;1.1.2 &lt;strong&gt;ndarrayÁöÑÊï∞ÊçÆÁ±ªÂûã&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Êï∞ÊçÆÁ±ªÂûãÔºåÊòØdytpeÂ±ûÊÄßÔºå‰ª£Ë°®‰∫ÜÊï∞ÁªÑÈáåÈù¢ÊØè‰∏™Êï∞ÊçÆÁöÑÁ±ªÂûã„ÄÇ&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Â∏∏ËßÅÁöÑÁ±ªÂûã&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#ÊúâÁ¨¶Âè∑ÁöÑ64‰ΩçÊï¥Êï∞&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;uint64&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#Êó†Á¨¶Âè∑ÁöÑ64‰ΩçÊï¥Êï∞&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;float64&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# ÂçïÁ≤æÂ∫¶ÊµÆÁÇπÊï∞„ÄÇÂÖºÂÆπcËØ≠Ë®ÄÁöÑfloat&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;float64&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# ÂèåÁ≤æÂ∫¶ÊµÆÁÇπÊï∞„ÄÇÂÖºÂÆπcËØ≠Ë®ÄÁöÑdouble&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;comple&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#64‰ΩçÁöÑÂ§çÊï∞&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;boll&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#Â∏ÉÂ∞îÁ±ªÂûã&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;object&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#ÂØπË±°Á±ªÂûã&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;string_&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#Â≠óÁ¨¶‰∏≤Á±ªÂûã&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;ÂèØ‰ª•‰ΩøÁî®&lt;code&gt;astype&lt;/code&gt;Êù•ËΩ¨Êç¢Êï∞ÊçÆÁöÑÁ±ªÂûãÔºå‰ΩøÁî®‰ªñËΩ¨Êç¢Êó∂‰ºöÁîüÊàê‰∏Ä‰∏™ÂÖ®Êñ∞ÁöÑÊï∞ÁªÑÂØπË±°„ÄÇ&lt;/p&gt;&#xA;&lt;h3 id=&#34;12-Âü∫Êú¨ËøêÁî®&#34;&gt;&#xA;  1.2 Âü∫Êú¨ËøêÁî®&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#12-%e5%9f%ba%e6%9c%ac%e8%bf%90%e7%94%a8&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;1.2.1 &lt;strong&gt;ÁÆóÊï∞ËøêÁÆó&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
