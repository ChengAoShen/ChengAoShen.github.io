<!DOCTYPE html>
<html lang="en">

<head>
  <title>
  ü§óIntroduction to Generative Models ¬∑ ChengAo Shen
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="ChengAo Shen">
<meta name="description" content="Generative Models are part of unsupervised learning models that can learned from the datasets without any labels. Unlike other unsupervised models to manipulate, denoise, interpolate between, or compress examples, generative models focus on generating plausible new samples having similar properties to the dataset.

Latent variable models: mapping the data examples $\mathbf{x}$ to unseen latent variables $\mathbf{z}$ which can capture the underlying structure in the dataset.">
<meta name="keywords" content="ChengAo Shen, Homepage, University of Houston, Personal Blog, Ê≤àÈ™ãÈ™ú, ‰∏™‰∫∫‰∏ªÈ°µ, ÂçöÂÆ¢">



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="ü§óIntroduction to Generative Models">
  <meta name="twitter:description" content="Generative Models are part of unsupervised learning models that can learned from the datasets without any labels. Unlike other unsupervised models to manipulate, denoise, interpolate between, or compress examples, generative models focus on generating plausible new samples having similar properties to the dataset.
Latent variable models: mapping the data examples $\mathbf{x}$ to unseen latent variables $\mathbf{z}$ which can capture the underlying structure in the dataset.">

<meta property="og:url" content="https://chengaoshen.com/en/posts/introduction-to-generative-models/">
  <meta property="og:site_name" content="ChengAo Shen">
  <meta property="og:title" content="ü§óIntroduction to Generative Models">
  <meta property="og:description" content="Generative Models are part of unsupervised learning models that can learned from the datasets without any labels. Unlike other unsupervised models to manipulate, denoise, interpolate between, or compress examples, generative models focus on generating plausible new samples having similar properties to the dataset.
Latent variable models: mapping the data examples $\mathbf{x}$ to unseen latent variables $\mathbf{z}$ which can capture the underlying structure in the dataset.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="en">
    <meta property="article:published_time" content="2024-08-26T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-08-26T00:00:00+00:00">




<link rel="canonical" href="https://chengaoshen.com/en/posts/introduction-to-generative-models/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.2cd2ba040eaadea1390f8199e24bd994fabd69b5a7034b43fc2440c58fd09808.css" integrity="sha256-LNK6BA6q3qE5D4GZ4kvZlPq9abWnA0tD/CRAxY/QmAg=" crossorigin="anonymous" media="screen" />






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css" integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin="anonymous" media="screen" />
  



 




<link rel="icon" type="image/svg+xml" href="/images/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">

<link rel="icon" type="image/png" href="/images/favicon.png">








</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="https://chengaoshen.com/">
      ChengAo Shen
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/en/posts/">Posts</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/en/news/">News</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/en/publications/">Publications</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/en/about/">About Me</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container page">
  <article>
    <header>
      <h1 class="title">
        <a class="title-link" href="https://chengaoshen.com/en/posts/introduction-to-generative-models/">
          ü§óIntroduction to Generative Models
        </a>
      </h1>
    </header>

    <p>Generative Models are part of unsupervised learning models that can learned from the datasets without any labels. Unlike other unsupervised models to manipulate, denoise, interpolate between, or compress examples, generative models focus on generating plausible new samples having similar properties to the dataset.</p>
<p><img src="https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20231025211322464.png" alt="Taxonomy of unsupervised learning models"></p>
<p><strong>Latent variable models</strong>: mapping the data examples $\mathbf{x}$ to unseen latent variables $\mathbf{z}$ which can capture the underlying structure in the dataset.</p>
<p>In this essay, we will introduce the categories of generative models, discuss their properties, and talk about how to measure them.</p>
<h2 id="what-are-probabilistic-models">
  What are probabilistic models?
  <a class="heading-link" href="#what-are-probabilistic-models">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>Before we dive into the typical forms of generative models, we should understand two major categories of them.</p>
<ul>
<li>
<p>Direct generative models, such as generative adversarial models, aim to provide a mechanism for generating samples similar to observed data ${\mathbf{x}_i}$</p>
</li>
<li>
<p>Probabilistic models learn the probability distribution over the train data. These models will assign a probability $Pr(\mathbf{x}|\phi)$ to each data point $\mathbf{x}$. The models aim to maximize the probability of the observed data ${\mathbf{x}_i}$.
</p>
$$
  L[\phi]=-\sum_{i=1}^{I}\mathrm{log}[Pr(\mathbf{x}_i|\phi)]
  $$</li>
</ul>
<p>There is an image to describe the training process of the two models.</p>
<p><img src="https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/Training%20of%20generative%20models.png" alt="Training of generative models"></p>
<h2 id="properties-of-generative-models">
  Properties of Generative Models
  <a class="heading-link" href="#properties-of-generative-models">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>There are several properties that Generative Models need to have:</p>
<ul>
<li>Efficient sampling: using less computational consumption.</li>
<li>High-quality sampling: output is indistinguishable from train data</li>
<li>Coverage: samples should represent the entire training distribution</li>
<li>Well-behaved latent space: change in latent space will perform in data similarly.</li>
<li>Efficient likelihood computation: able to calculate the probability of new examples efficiently.</li>
</ul>
<p>It‚Äôs hard for only one type of Generative Model to obtain all properties, following is a table to describe some generative model‚Äôs features:</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Model</th>
          <th style="text-align: center">Probabilistic</th>
          <th style="text-align: center">Efficient</th>
          <th style="text-align: center">Sample quality</th>
          <th style="text-align: center">Coverage</th>
          <th style="text-align: center">Well-behaved latent space</th>
          <th style="text-align: center">Disentangled latent space</th>
          <th style="text-align: center">Efficient likelihood</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center"><strong>GANs</strong></td>
          <td style="text-align: center">‚úñÔ∏è</td>
          <td style="text-align: center">‚úîÔ∏è</td>
          <td style="text-align: center">‚úîÔ∏è</td>
          <td style="text-align: center">‚úñÔ∏è</td>
          <td style="text-align: center">‚úîÔ∏è</td>
          <td style="text-align: center">‚ùî</td>
          <td style="text-align: center">\</td>
      </tr>
      <tr>
          <td style="text-align: center"><strong>Flows</strong></td>
          <td style="text-align: center">‚úîÔ∏è</td>
          <td style="text-align: center">‚úîÔ∏è</td>
          <td style="text-align: center">‚úñÔ∏è</td>
          <td style="text-align: center">‚ùî</td>
          <td style="text-align: center">‚úîÔ∏è</td>
          <td style="text-align: center">‚ùî</td>
          <td style="text-align: center">‚úñÔ∏è</td>
      </tr>
      <tr>
          <td style="text-align: center"><strong>VAEs</strong></td>
          <td style="text-align: center">‚úîÔ∏è</td>
          <td style="text-align: center">‚úîÔ∏è</td>
          <td style="text-align: center">‚úñÔ∏è</td>
          <td style="text-align: center">‚ùî</td>
          <td style="text-align: center">‚úîÔ∏è</td>
          <td style="text-align: center">‚ùî</td>
          <td style="text-align: center">‚úîÔ∏è</td>
      </tr>
      <tr>
          <td style="text-align: center"><strong>Diffusion</strong></td>
          <td style="text-align: center">‚úîÔ∏è</td>
          <td style="text-align: center">‚úñÔ∏è</td>
          <td style="text-align: center">‚úîÔ∏è</td>
          <td style="text-align: center">‚ùî</td>
          <td style="text-align: center">‚úñÔ∏è</td>
          <td style="text-align: center">‚úñÔ∏è</td>
          <td style="text-align: center">‚úñÔ∏è</td>
      </tr>
  </tbody>
</table>
<h2 id="performance-measurement">
  Performance Measurement
  <a class="heading-link" href="#performance-measurement">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>To measure the performance of generative models, some metrics are proposed.</p>
<p><strong>Inception score (IS)</strong>: This metric is used in image generative models which trained on the ImageNet dataset. There are two criteria to design it. First, each generated image $\mathbf{x}^*$ should look like only one class in the ImageNet dataset which has 1000 possible classes. Second, the probability for each class in generated images should be equal.</p>
<p><img src="https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20231026144638161.png" alt="Inception score"></p>
<p>This metric uses KL divergence between $Pr(y_i|\mathbf{x}_i^*)$and $Pr(y)$
</p>
$$
IS=\mathrm{exp}\bigg[\frac{1}{I}\sum_{i=1}^{I}D_{KL}\big[Pr(y_i|\mathbf{x}_i^*)||Pr(y)\big]\bigg]
$$<p>
Where $I$ is the number of generated images and:
</p>
$$
Pr(y)=\frac{1}{I}\sum_{i=1}^{I}Pr(y_i|\mathbf{x}_i^*)
$$<p>
<strong>Fr√©chet inception distance</strong>: To decrease the reliance on the ImageNet dataset and characterize either distribution, the Fr√©chet inception distance is estimated.</p>
<p>First, use the inception model accepting both observed and generated images as input to produce $1\times 2048$ feature vector. And, because the images follow the normal distribution which can be defined by mean and variance, we can use them to calculate the distance between real and generated images.</p>
<p>Above all, the formula for calculating FID can be written as follows:
</p>
$$
FID(x,g)=||\mu_x-\mu_g||_2^2+Tr\big(\Sigma_x+\Sigma_g-2(\Sigma_x\Sigma_g)^{0.5}\big)
$$<p>
Where $x,g$ present real and generated images, $\mu$ is the mean and $\Sigma$ is the covariance.</p>
<p>However, this metric uses the inception network output to calculate, which will more focus on the semantic information.</p>
<p><strong>Manifold precision/recall</strong>: To disentangle the realism of the samples and their diversity, we consider the overlap between the data manifold and the model manifold.</p>
<ul>
<li>Precision is the fraction of <strong>model samples</strong> that fall into the <strong>data manifold</strong>.</li>
<li>Recall is the fraction of <strong>data examples</strong> that fall within the <strong>model manifold</strong>.</li>
</ul>
<h2 id="reference">
  Reference
  <a class="heading-link" href="#reference">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>[1] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, ‚ÄòRethinking the Inception Architecture for Computer Vision‚Äô, in <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2016.</p>
<p>[2] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, ‚ÄòGANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium‚Äô, in <em>Advances in Neural Information Processing Systems</em>, 2017, vol. 30.</p>
<p>[3] S. J. D. Prince, <em>Understanding Deep Learning</em>. MIT Press, 2023.</p>

  </article>
</section>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css"
    integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
  
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"
    integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body,
      {
        delimiters: [
          {left: '$$', right: '$$', display:true},
          {left: '$', right: '$', display:false},
          {left: '\\(', right: '\\)', display: false},
          {left: '\\[', right: '\\]', display: true}
        ]
      }
    );"></script>

    </div>

    <footer class="footer">
  <section class="container">
    ¬©
    
      2024 -
    
    2025
     ChengAo Shen 
    ¬∑
    
    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js" integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>
</html>
