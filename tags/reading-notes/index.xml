<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reading Notes | ChengAo Shen</title>
    <link>https://chengaoshen.com/tags/reading-notes/</link>
      <atom:link href="https://chengaoshen.com/tags/reading-notes/index.xml" rel="self" type="application/rss+xml" />
    <description>Reading Notes</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 09 Jul 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://chengaoshen.com/media/icon_hu13846342205060721621.png</url>
      <title>Reading Notes</title>
      <link>https://chengaoshen.com/tags/reading-notes/</link>
    </image>
    
    <item>
      <title>ðŸ“ŠReading Notes for &#34;LEMMA-RCA&#34;</title>
      <link>https://chengaoshen.com/blogs/reading-notes-for-lemma-rca/</link>
      <pubDate>Tue, 09 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/blogs/reading-notes-for-lemma-rca/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;This paper introduces a new large dataset named &lt;strong&gt;LEMMA-RCA&lt;/strong&gt; for diverse RCA tasks across multiple domains and modalities. This dataset contains IT and OT operation systems from the real world. They also evaluate eight baseline methods on this dataset to prove the high quality of LEMMA_RCA. The official website is &lt;a href=&#34;https://lemma-rca.github.io/&#34;&gt;https://lemma-rca.github.io/&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;What problem does the paper try to solveï¼Ÿ&lt;/p&gt;
&lt;p&gt;The use of automated methods for root cause analysis is crucial, but currently, there is a lack of a mainstream dataset and fair comparison is not possible.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is the proposed solution?&lt;/p&gt;
&lt;p&gt;They proposed a rich dataset LEMMA-RCA containing multiple sub-datasets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What are the key experimental results in this paper?&lt;/p&gt;
&lt;p&gt;Tested the performance of 8 models on the LEMMA-RCA dataset.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What are the main contributions of the paper?&lt;/p&gt;
&lt;p&gt;They propose the LEMMA-RCA dataset and evaluate eight baseline models on this.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What are the strong points and weak points in this paper?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Strong Point: Proposed a new dataset and conducted extensive evaluation.&lt;/li&gt;
&lt;li&gt;Weak Point: There are no baseline methods not belonging to the causal-graph-based model.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;Root cause analysis (RCA) is essential for identifying the underlying causes of system failures and ensuring the reliability and robustness of real-world systems. However, traditional manual RCA is labor-intensive, costly, and prone to errors, so data-driven methods are needed. Despite significant progress in RCA techniques, &lt;strong&gt;the large-scale public datasets remain limited&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In RCA fields, here are some important keywords:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Key Performance Indicator (KPI)&lt;/strong&gt; is a time series indicating the system status, such as latency and service response time in microservice systems.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Entity Metrics&lt;/strong&gt; are multivariate time series collected by monitoring numerous system entities or components, such as CPU/Memory utilization in microservice systems.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data-driven Root Cause Analysis Problem&lt;/strong&gt;. Given the monitoring data of system entities and system KPIs, identify the top K system entities that are relevant to KPIs when the system fails.
&lt;ul&gt;
&lt;li&gt;Offline/Online: Offline RCA only uses historical data to determine past failures; Online RCA operates in real-time using current data streams to promptly address issues.&lt;/li&gt;
&lt;li&gt;Single-modal/multi-modal: Single-modal RCA relies solely on one type of data for a focused analysis; Multi-modal RCA uses multiple data sources for a comprehensive assessment.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/RCA_workflow.png&#34; alt=&#34;RCA workflow&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;h3 id=&#34;base-information&#34;&gt;Base Information&lt;/h3&gt;
&lt;p&gt;LEMMA-RCA is a multi-domain, multi-modal dataset that includes textual system logs with millions of event records and time series metric data collected from real system faults. This dataset includes IT and OT scenes, such as microservice and water treatment.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/RCA_datasets.png&#34; alt=&#34;Existing datasets for root cause analysis.&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;collection&#34;&gt;Collection&lt;/h3&gt;
&lt;p&gt;The dataset collected from two domains, divided into four sub-datasets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;IT operations&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Product Review&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Platform&lt;/strong&gt;: Composed of six OpenShift nodes and 216 system pods.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20240710000718711.png&#34; alt=&#34;The architecture of Product Review Platform&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Faults&lt;/strong&gt;: out-of-memory, high-CPU-usage, external-storage-full, DDos attack.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Metrics&lt;/strong&gt;: Using Prometheus to record eleven types of node-level metrics and six types of pod-level metrics; Using ElasticSearch to collect log data including timestamp, pod name, log message, etc; Using JMeter to collect the system status information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;KPI&lt;/strong&gt;: Consider latency as system KPI due to system failure will result in latency significantly increasing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cloud Computing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Platform&lt;/strong&gt;: Eleven system nodes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Faults&lt;/strong&gt;: six different types of faults, such as cryptojacking, configuration change failure, etc.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Metrics&lt;/strong&gt;: Extracting system metrics from CloudWatch Metrics on EC2 instances; Extracting three logs types (log messages, API debug log, and MySQL log) from CloudWatch Logs; Using JMeter tools to record error rate and utilization rate as KPIs.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/IT_Sub.png&#34; alt=&#34;Data statistics of IT operation sub-datasets.&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OT operations&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SWaT: Collected over an 11-day period from a water treatment testbed equipped with 51 sensors. The system operated normally during the first 7 days, followed by attacks over the last 4 days, resulting in 16 system faults.&lt;/li&gt;
&lt;li&gt;WADI: Gathered from a water distribution testbed over 16 days, featuring 123 sensors and actuators. The system maintained normal operations for the first 14 days before experiencing attacks in the final 2 days, with 15 system faults recorded.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/OT_sub.png&#34; alt=&#34;Data statistics of OT operation sub-datasets.&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;preprocessing&#34;&gt;Preprocessing&lt;/h3&gt;
&lt;p&gt;Some non-stationary data are unpredictable and cannot be effectively modeled, which means they should be excluded. Thus this paper introduces some methods to preprocessing the data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Log Feature Extraction&lt;/strong&gt;. Due to the log data being unstructured and some of them being unmeaning,  this paper transforms the log data into the time-series format. First, they use log-parsing tools to structure the log message. Then they segment the data using &lt;em&gt;10-minute windows&lt;/em&gt; with &lt;em&gt;30-second intervals&lt;/em&gt; and calculate the occurrence frequency as the first feature type donated as $X_1^L\in \mathbb{R}^T$. Then, they introduce a second feature type based on &amp;ldquo;Golden signals&amp;rdquo; derived from domain knowledge, such as the frequency of abnormal logs associated with system failures like DDoS attacks, storage failures, and resource over-utilization. This feature is donated as $X_2^L\in \mathbb{R}^T$. Finally, they segment the log using the same time windows and apply PCA to reduce feature dimensionality, selecting the most significant component as $X_3^L\in \mathbb{R}^T$. The overall data can form as matrix $X^L=[X_1^L,X_2^L,X_3^L]\in \mathbb{R}^{3\times T}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;KPI Construction&lt;/strong&gt;. Using anomaly detection algorithms to model the SWaT and WADI datasets, and transform the discrete value into continuous format.&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;metrics&#34;&gt;Metrics&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Precision@K (PR@K)&lt;/strong&gt;: It measures the probability that the top $K$ predicted root causes are real, formulated as:&lt;/p&gt;
$$
\text{PR@K}=\frac{1}{|\mathbb{A}|}\sum_{a\in\mathbb{A}}\frac{\sum_{i\le k}R_a(i)\in V_a}{\text{min}(K,|v_a|)}
$$&lt;p&gt;Where $\mathbb{A}$ is the set of system faults, $a$ is one fault, $V_a$ is the real root cause of $a$, $R_a$ is the predicted root cause of $a$, and $i$ is the $i$-th predicted cause of $R_a$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mean Average Precision@K (MAP@K)&lt;/strong&gt;: It assesses the top $K$ predicted causes from the overall perspective, formulated as:&lt;/p&gt;
$$
\text{MAP@K}=\frac{1}{K|\mathbb{A}|}\sum_{a\in \mathbb{A}}\sum_{i\le j\le K}\text{PR@j}
$$&lt;p&gt;&lt;strong&gt;Mean Reciprocal Rank (MRR)&lt;/strong&gt;: It evaluates the ranking capability of models, formulated as:&lt;/p&gt;
$$
\text{MRR@K}=\frac{1}{|\mathbb{A}|}\sum_{a\in \mathbb{A}}\frac{1}{\text{rank}_{R_a}}
$$&lt;p&gt;Where $\text{rank}_{R_{a}}$ is the rank number of the first correctly predicted root cause for system fault $a$.&lt;/p&gt;
&lt;h3 id=&#34;baselines&#34;&gt;Baselines&lt;/h3&gt;
&lt;p&gt;Causal-graph-based RCA methods can provide deeper insights into system failures, thus all baseline methods fall into this category.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PC: Classic constrain-based causal discovery algorithm that can identify the causal graph&amp;rsquo;s skeleton using an independence test.&lt;/li&gt;
&lt;li&gt;Dynotears: It construct dynamic Bayesian networks through vector autoregression models.&lt;/li&gt;
&lt;li&gt;C-LSTM: Utilizes LSTM to model temporal dependencies and capture nonlinear Granger causality.&lt;/li&gt;
&lt;li&gt;GOLEM:  relaxing the hard Directed Acyclic Graph (DAG) constraint of NOTEARS with a scoring function&lt;/li&gt;
&lt;li&gt;REASON: An interdependent network model learning both intra-level and inter-level causal relationships.&lt;/li&gt;
&lt;li&gt;Nezha: A multi-modal method designed to identify root causes by detecting abnormal patterns.&lt;/li&gt;
&lt;li&gt;MULAN: A multi-modal RCA method that learns the correlation between different modalities and co-constructs a causal graph for root cause identification&lt;/li&gt;
&lt;li&gt;CORAL: An online single-modal RCA method based on incremental disentangled causal graph learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/RCA_Product.png&#34; alt=&#34;Results for offline RCA baselines with multiple modalities on the Product Review dataset.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/RCA_SAT.png&#34; alt=&#34;Results for offline RCA baselines on the SWaT and WADI dataset.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/RCA.png&#34; alt=&#34;Results for online root cause analysis baselines on all sub-datasets.&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ðŸ“‘Reading Notes for &#34;LTSF-Linear&#34;</title>
      <link>https://chengaoshen.com/blogs/reading-notes-for-ltsf-linear/</link>
      <pubDate>Sat, 22 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/blogs/reading-notes-for-ltsf-linear/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;This paper insists that the time series is an ordered set of continuous points that will result in the loss of temporal information when using the Transformer structure. To prove this opinion, they propose models named &lt;strong&gt;LSTF-Linear&lt;/strong&gt; which achieve outstanding performance and conduct comprehensive studies.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;What problem does the paper try to solveï¼Ÿ&lt;/p&gt;
&lt;p&gt;Attempt to verify whether the Transformer is effective in time series prediction problems and whether a simple model can surpass all current Transformer-based models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is the proposed solution?&lt;/p&gt;
&lt;p&gt;The LSTF Linear model was proposed as an alternative, achieving extremely high performance with only a single-layer network&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What are the key experimental results in this paper?&lt;/p&gt;
&lt;p&gt;Achieved better performances than Transformer-based methods on multiple datasets such as electricity, healthcare, and meteorology&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What are the main contributions of the paper?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Challenge the Transformer structure in the long-term time series forecasting task.&lt;/li&gt;
&lt;li&gt;Introduce the LTSF-Linear model which only has one layer while achieving compared results in various fields.&lt;/li&gt;
&lt;li&gt;Conduct comprehensive empirical studies on various aspects of existing Transformer-based solutions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What are the strong points and weak points in this paper?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Strong Points&lt;/strong&gt;ï¼š Proposed potential issues in the current research route and opened up a new perspective in a simple way.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Weak Points&lt;/strong&gt;: Only conducted research on prediction problems and have not explored other issues, such as anomaly detection.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;Over the past several decades, the Transformer has been widely used as the TSF solution. However, the self-attention mechanism is &lt;strong&gt;permutation-invariant&lt;/strong&gt; and &lt;strong&gt;anti-order&lt;/strong&gt; which will cause the loss of temporal information. Typically, time series contain less semantic meaning compared with NLP or CV problems and need more temporal information, which will emphasize this problem. Thus, this paper tries to challenge the Transformer-based LSTF solution with direct multi-step forecasting strategies.&lt;/p&gt;
&lt;p&gt;For the time series containing $C$ variates, the historical data can be represented as $\mathcal{X}=\{X_1^t,\dots,X_C^t\}_{t=1}^L$, wherein $L$ is the look-back window size. The forecasting problems need to predict $T$ feature time stepâ€™s value $\hat{\mathcal{X}}=\{\hat{X}_1^t,\dots,\hat{X}_C^t\}_{t=1}^L$. When $T&gt;1$, the methods can be divided into two parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Integrated multi-step(IMS): Learn a single-step forecaster and interactively apply it to obtain multi-step predictions. This method has a smaller variance but will cause error accumulation.&lt;/li&gt;
&lt;li&gt;Direct multi-step(DMS): Directly optimize the multi-step forecasting objective at one. This method can have more accurate predictions when $T$ is large.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;methods&#34;&gt;Methods&lt;/h2&gt;
&lt;h3 id=&#34;transformer-based-methods&#34;&gt;Transformer-Based Methods&lt;/h3&gt;
&lt;p&gt;The vanilla Transformer model has some limitations when applied to time series problems, thus various works try to improve the performance by adding or replacing some parts of the Transformer. Generally speaking, it can be divided into four major parts.&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20240622212351839.png&#34; alt=&#34;The pipeline of Transformer-Based Methods&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;The pipeline of Transformer-Based Methods&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Preprocessing&lt;/strong&gt;: To use Transformer to deal with time series datasets, some preprocessing is needed to adopt the data structures, such as normalization with zero-mean and adding timestamps as NLP did. Specifically, in Autoformer, seasonal-trend decomposition is proposed to get the trend part and the cyclical part, which helps data more clearly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Embedding&lt;/strong&gt;: In NLPâ€™s Transformer, embedding will map the words to the vector in a typical space that reveals the meaning. In time series, time information is significantly important. Thus, various timestamp methods are proposed to help the model reserve the temporal information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Encoder/Decoder&lt;/strong&gt;: To help the Transformer structure adopt the time series problems, there have been many improvements made to the encoder and decoder in this work. Among them, in the encoder section, many improvements have been proposed to reduce computational consumption and increase speed. In the decoder section, to avoid cumulative errors, it has also begun to transition from IMS to DMS.&lt;/p&gt;
&lt;p&gt;The success of Transformer in the NLP field is largely attributed to its understanding of the semantic relationships between words, but in time series problems, temporary information has become even more important. However, the Transformer&amp;rsquo;s ability to model time largely comes from the timestamp rather than its structure.&lt;/p&gt;
&lt;h3 id=&#34;lstf-linear&#34;&gt;LSTF-Linear&lt;/h3&gt;
&lt;p&gt;This paper hypothesizes that the improvement in the Transformer-Based model is due to the DMS strategy rather than the Transformer. Thus they propose LSTF-Linear that directly regresses historical time series for future prediction via a weighted sum operation to verify this thinking. The model can be a formula as :
&lt;/p&gt;
$$
\hat{X}_i=WX_i
$$&lt;p&gt;
Wherein $W\in\mathbb{R}^{T\times L}$ is a linear layer along the temporal axis. Furthermore, this paper proposes two sub-models, DLinear which uses decomposition to obtain the trend part and the seasonal part, and the NLinear which subtracts a portion before making the prediction and rejoined it after making the prediction.&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20240622212512231.png&#34; alt=&#34;The Structure of LSTF-Linear&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;The Structure of LSTF-Linear&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;In order to verify the quality of the LSTF-Linear, the author selected some common sequence data from real life and compared with five popular Transformer-based model. All results are show as follow:&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20240623194839137.png&#34; alt=&#34;Datasets Description&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;Datasets Description&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20240623194933447.png&#34; alt=&#34;Model Comparison&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;Model Comparison&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20240623195006823.png&#34; alt=&#34;Visualize results&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;Visualize results&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;figure&#34;&gt;
  &lt;img class=&#34;img&#34; src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20240623200500165.png&#34; alt=&#34;MSE in different look-back windows size&#34;&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;MSE in different look-back windows size&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
