<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | ChengAo Shen</title>
    <link>https://chengaoshen.com/tags/deep-learning/</link>
      <atom:link href="https://chengaoshen.com/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 26 Oct 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://chengaoshen.com/media/icon_hu68170e94a17a2a43d6dcb45cf0e8e589_3079_512x512_fill_lanczos_center_3.png</url>
      <title>Deep Learning</title>
      <link>https://chengaoshen.com/tags/deep-learning/</link>
    </image>
    
    <item>
      <title>ü§óIntroduction to Generative Models</title>
      <link>https://chengaoshen.com/blogs/introduction-to-generative-models/</link>
      <pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/blogs/introduction-to-generative-models/</guid>
      <description>&lt;p&gt;Generative Models are part of unsupervised learning models that can learned from the datasets without any labels. Unlike other unsupervised models to manipulate, denoise, interpolate between, or compress examples, generative models focus on generating plausible new samples having similar properties to the dataset.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20231025211322464.png&#34; alt=&#34;Taxonomy of unsupervised learning models&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Latent variable models&lt;/strong&gt;: mapping the data examples $\mathbf{x}$ to unseen latent variables $\mathbf{z}$ which can capture the underlying structure in the dataset.&lt;/p&gt;
&lt;p&gt;In this essay, we will introduce the categories of generative models, discuss their properties, and talk about how to measure them.&lt;/p&gt;
&lt;h2 id=&#34;what-are-probabilistic-models&#34;&gt;What are probabilistic models?&lt;/h2&gt;
&lt;p&gt;Before we dive into the typical forms of generative models, we should understand two major categories of them.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Direct generative models, such as generative adversarial models, aim to provide a mechanism for generating samples similar to observed data $\{\mathbf{x}_i\}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Probabilistic models learn the probability distribution over the train data. These models will assign a probability $Pr(\mathbf{x}|\phi)$ to each data point $\mathbf{x}$. The models aim to maximize the probability of the observed data $\{\mathbf{x}_i\}$.
&lt;/p&gt;
$$
  L[\phi]=-\sum_{i=1}^{I}\mathrm{log}[Pr(\mathbf{x}_i|\phi)]
  $$
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is an image to describe the training process of the two models.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/Training%20of%20generative%20models.png&#34; alt=&#34;Training of generative models&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;properties-of-generative-models&#34;&gt;Properties of Generative Models&lt;/h2&gt;
&lt;p&gt;There are several properties that Generative Models need to have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Efficient sampling: using less computational consumption.&lt;/li&gt;
&lt;li&gt;High-quality sampling: output is indistinguishable from train data&lt;/li&gt;
&lt;li&gt;Coverage: samples should represent the entire training distribution&lt;/li&gt;
&lt;li&gt;Well-behaved latent space: change in latent space will perform in data similarly.&lt;/li&gt;
&lt;li&gt;Efficient likelihood computation: able to calculate the probability of new examples efficiently.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It‚Äôs hard for only one type of Generative Model to obtain all properties, following is a table to describe some generative model‚Äôs features:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Model&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Probabilistic&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Efficient&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Sample quality&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Coverage&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Well-behaved latent space&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Disentangled latent space&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Efficient likelihood&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;GANs&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úñÔ∏è&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úîÔ∏è&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úîÔ∏è&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úñÔ∏è&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úîÔ∏è&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚ùî&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;\&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;Flows&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úîÔ∏è&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úîÔ∏è&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úñÔ∏è&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚ùî&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úîÔ∏è&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚ùî&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úñÔ∏è&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;VAEs&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úîÔ∏è&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úîÔ∏è&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úñÔ∏è&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚ùî&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úîÔ∏è&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚ùî&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úîÔ∏è&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;strong&gt;Diffusion&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úîÔ∏è&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úñÔ∏è&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úîÔ∏è&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚ùî&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úñÔ∏è&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úñÔ∏è&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;‚úñÔ∏è&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;performance-measurement&#34;&gt;Performance Measurement&lt;/h2&gt;
&lt;p&gt;To measure the performance of generative models, some metrics are proposed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inception score (IS)&lt;/strong&gt;: This metric is used in image generative models which trained on the ImageNet dataset. There are two criteria to design it. First, each generated image $\mathbf{x}^*$ should look like only one class in the ImageNet dataset which has 1000 possible classes. Second, the probability for each class in generated images should be equal.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20231026144638161.png&#34; alt=&#34;Inception score&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This metric uses KL divergence between $Pr(y_i|\mathbf{x}_i^*)$and $Pr(y)$
&lt;/p&gt;
$$
IS=\mathrm{exp}\bigg[\frac{1}{I}\sum_{i=1}^{I}D_{KL}\big[Pr(y_i|\mathbf{x}_i^*)||Pr(y)\big]\bigg]
$$
&lt;p&gt;
Where $I$ is the number of generated images and:
&lt;/p&gt;
$$
Pr(y)=\frac{1}{I}\sum_{i=1}^{I}Pr(y_i|\mathbf{x}_i^*)
$$
&lt;p&gt;
&lt;strong&gt;Fr√©chet inception distance&lt;/strong&gt;: To decrease the reliance on the ImageNet dataset and characterize either distribution, the Fr√©chet inception distance is estimated.&lt;/p&gt;
&lt;p&gt;First, use the inception model accepting both observed and generated images as input to produce $1\times 2048$ feature vector. And, because the images follow the normal distribution which can be defined by mean and variance, we can use them to calculate the distance between real and generated images.&lt;/p&gt;
&lt;p&gt;Above all, the formula for calculating FID can be written as follows:
&lt;/p&gt;
$$
FID(x,g)=||\mu_x-\mu_g||_2^2+Tr\big(\Sigma_x+\Sigma_g-2(\Sigma_x\Sigma_g)^{0.5}\big)
$$
&lt;p&gt;
Where $x,g$ present real and generated images, $\mu$ is the mean and $\Sigma$ is the covariance.&lt;/p&gt;
&lt;p&gt;However, this metric uses the inception network output to calculate, which will more focus on the semantic information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Manifold precision/recall&lt;/strong&gt;: To disentangle the realism of the samples and their diversity, we consider the overlap between the data manifold and the model manifold.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Precision is the fraction of &lt;strong&gt;model samples&lt;/strong&gt; that fall into the &lt;strong&gt;data manifold&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Recall is the fraction of &lt;strong&gt;data examples&lt;/strong&gt; that fall within the &lt;strong&gt;model manifold&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, ‚ÄòRethinking the Inception Architecture for Computer Vision‚Äô, in &lt;em&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, 2016.&lt;/p&gt;
&lt;p&gt;[2] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, ‚ÄòGANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium‚Äô, in &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, 2017, vol. 30.&lt;/p&gt;
&lt;p&gt;[3] S. J. D. Prince, &lt;em&gt;Understanding Deep Learning&lt;/em&gt;. MIT Press, 2023.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üí°Introduction to Transformer</title>
      <link>https://chengaoshen.com/blogs/introduction-to-transformer/</link>
      <pubDate>Mon, 23 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://chengaoshen.com/blogs/introduction-to-transformer/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Transformer is a really popular method in modern neural networks. We have BERT or GPT to process the natural language and ViT to deal with computer vision. In this essay, you will understand what is the transformer and why the transformer works. But be careful, limited by my knowledge, I can‚Äôt show some mathematical theories or code of transformer for you.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;why-do-we-need-the-transformer&#34;&gt;Why do we need the Transformer?&lt;/h2&gt;
&lt;p&gt;In the NLP( Natural Language Processing) field, the text dataset always has some obvious features that prevent us from using MLP.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Too large after being encoder.&lt;/p&gt;
&lt;p&gt;To represent the words, we should embed them into vectors first. Generally, we use the vector of length 1024 to describe one word, meaning the data size will become large after multiplying the size of the vector(1024).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Having different length&lt;/p&gt;
&lt;p&gt;Our inputs for the NLP problems have various lengths according to the size of passages or sentences. We should build models that adopt different size inputs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ambiguous&lt;/p&gt;
&lt;p&gt;The texts don&amp;rsquo;t resemble the numbers having certain meanings from themselves. Some words like ‚Äúit‚Äù refer to others in the context, which means they are ambiguous in different backgrounds.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To sum up, the features or problems we mention above prevent us from using normal structures like MLP to process natural language. So we need new things - Transformer. Let us approach this by self-attention first.&lt;/p&gt;
&lt;h2 id=&#34;self-attention&#34;&gt;Self-attention&lt;/h2&gt;
&lt;h3 id=&#34;parameter-sharing&#34;&gt;Parameter sharing&lt;/h3&gt;
&lt;p&gt;A standard neural network layer $f[x]$, take $D\times 1$ inputs and return $D&#39;\times 1$. When we process embedding text, our input‚Äôs dimensions are changed to $D\times N$. To hold enough information in our model, we assume the size of our data remains unchanged, which means the size of outputs is also $D\times N$. Where $D$ is the size of the embedding vector, $N$ is the number of the word.&lt;/p&gt;
&lt;p&gt;Roughly, different words have their meanings just like the dictionary shows, which inspires us to use the neural network to approximate the dictionary - mapping the words to their abstract meanings described by numbers.
&lt;/p&gt;
$$
v=\beta_v+\Omega_vx
$$
&lt;p&gt;Where $x$ is embedding word, $v$ is abstract meaning of word(present by number), and $\beta_v,\Omega_v$ is the parameter mapping the words to their meanings.&lt;/p&gt;
&lt;p&gt;As we can use one dictionary to map every word to their meanings, we can just use $\beta_v,\Omega_v$ to map every embedding word, called &lt;strong&gt;parameter sharing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Normally, the inputs and outputs are represented as $D\times N$ matrix, so we can rewrite our equation as follows
&lt;/p&gt;
$$
V[X]=\beta_v1^T+\Omega_vX
$$
&lt;h3 id=&#34;from-weight-to-self-attention&#34;&gt;From weight to self-attention&lt;/h3&gt;
&lt;p&gt;In the last section, we use ‚Äúdictionary‚Äù to map words, but the problem is we haven‚Äôt considered the context of the words. So, how can we let our ‚Äúdictionary‚Äù know the context?
One simple thinking is we can give every ‚Äúmeaning‚Äù a &lt;strong&gt;weight&lt;/strong&gt; and weighted sum the values.
&lt;/p&gt;
$$
O[X]=V[X]\times W
$$
$$
W=\begin{pmatrix}
 w_1 &amp;\dots  &amp;w_n \\
 \vdots  &amp;  &amp;\vdots  \\
 w_1 &amp; \dots &amp;w_n
\end{pmatrix}
$$
&lt;p&gt;How can we get this weight that measures the relationship between the words in the sentence? Inspired by the search system, we can use ‚Äúqueries‚Äù to match ‚Äúkey‚Äù. Same as we did before, we can just use linear transformation to get them.
&lt;/p&gt;
$$
\begin{split}
Q[X]=\beta_q1^T+\Omega_qX\\
K[X]=\beta_k1^T+\Omega_kX
\end{split}
$$
&lt;p&gt;Where $\beta_q,\Omega_q,\beta_k,\Omega_q$ are also shared for every word.&lt;/p&gt;
&lt;p&gt;Then, we can use $K$ to query the $Q$, telling us which word should pay how much &lt;strong&gt;attention&lt;/strong&gt; to others. Typically, the attention should sum up as $1$, so we can use the softmax function to achieve this. Because we use $X$ to pay attention to itself, we call this &lt;strong&gt;Self-attention&lt;/strong&gt;. To sum up, our model is described as follows:
&lt;/p&gt;
$$
Sa[X]=V[X]\cdot \text{Softmax}[K[X]^TQ[X]]
$$
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20230726112139933.png&#34; alt=&#34;Self-attention&#34;&gt;&lt;/p&gt;
&lt;p&gt;Typically, the dot products will be really large, which means the small changes to the inputs have nearly no effect on the output because we use the softmax function. So we always use the dimension of the input, always the shape of embedding, to scale the dot products.&lt;/p&gt;
$$
Sa[X]=V[X]\cdot \text{Softmax}[\frac{K[X]^TQ[X]}{\sqrt{D_q}}]
$$
&lt;h3 id=&#34;multiple-heads&#34;&gt;Multiple heads&lt;/h3&gt;
&lt;p&gt;To increase the information that model learning, we always use more attention units when we calculate the output. We will divide the input to $h$ parts, and calculate the self-attention with them. After that, we will combine all of them to recreate our output.
&lt;/p&gt;
$$
\text{MhSa}[X]=\Omega_c[Sa_1[X]^T,Sa_2[X]^T,\dots,Sa_H[X]^T]^T
$$
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20230726201710681.png&#34; alt=&#34;Multiple heads&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;transformer-layers&#34;&gt;Transformer layers&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChengAoShen/Image-Hosting/main/images/image-20230726113937045.png&#34; alt=&#34;Transformer layers&#34;&gt;&lt;/p&gt;
&lt;p&gt;With &lt;strong&gt;self-attention&lt;/strong&gt; and &lt;strong&gt;MLP&lt;/strong&gt;, we can construct the transformer layers applied in many outstanding models. The process looks like this:
&lt;/p&gt;
$$
\begin{split}
X \leftarrow X+\text{MhSa}[X]\\
X\leftarrow \text{LayerNorm}[X]\\
x_n\leftarrow x_n+\text{MLP}[x_n]\\
X\leftarrow \text{LayerNorm}[X]
\end{split}
$$
&lt;p&gt;Where $\text{MLP}$ is fully connected network works separately on each word and $\text{LayerNorm}$ is the normalization happens in the channel, like:&lt;/p&gt;
$$
y=\frac{x-E[x]}{\sqrt{Var[x]+\epsilon }}*\gamma+\beta
$$
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] S. J. D. Prince, &lt;em&gt;Understanding Deep Learning&lt;/em&gt;. MIT Press, 2023.&lt;/p&gt;
&lt;p&gt;[2] A. Vaswani &lt;em&gt;et al.&lt;/em&gt;, ‚ÄòAttention Is All You Need‚Äô, &lt;em&gt;CoRR&lt;/em&gt;, vol. abs/1706.03762, 2017.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;All photos in this article are from &lt;a href=&#34;https://udlbook.github.io/udlbook/&#34;&gt;Understanding Deep Learning&lt;/a&gt;. It‚Äôs a great book and I really recommend it&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>
